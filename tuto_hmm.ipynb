{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from submapp import map2d, som, hmm\n",
    "from tools.tools_som import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.1 Extract data from files\n",
    "\n",
    "There is nothing to change in this cell, just run the cell to extract data from the netcdf4 file. \n",
    "If this does not work, make sure the file ``GotmFabmErsem-BATS.nc`` is in the right folder. (It should be in the same folder as this notebook, ``submapp`` folder and ``tools`` folder )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ STEP 0.1 ##################################################\n",
    "# ---- Extracting data\n",
    "([temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth]) = data_extraction()\n",
    "gotm_data = [temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth]\n",
    "data_types = [\"temp\", \"sst\", \"dswr\", \"airt\", \"ws10\", \"depth\"]\n",
    "\n",
    "for i in range(len(gotm_data)-1):\n",
    "    for y in range(len(gotm_data[i])):\n",
    "        gotm_data[i][y] = mean_steps(gotm_data[i][y],10)\n",
    "[temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth] = gotm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.2 Select data types\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Select the data types you want to work with by initializing the list ``data_types_used`` with some of the following elements \n",
    "     - 0: Temperature (temp)\n",
    "     - 1: Sea surface temperature (sst)\n",
    "     - 2: Incoming Short Wave Radiation (dswr)\n",
    "     - 3: Air Temperature (airt)\n",
    "     - 4: Wind Speed (ws10)\n",
    "     - (-1): Depth (depth)\n",
    "     \n",
    "``Temperature (0)`` is related to the vertical profiles of temperature, in our case they are the \"*hidden*\" values.\n",
    "\n",
    "`` Depth (-1)`` is a vector of levels of depth corresponding to the vertical profiles\n",
    "\n",
    "``Sea surface temperature (1), Incoming Short Wave Radiation (2), Air Temperature (3)`` and ``Wind Speed (4)`` are surface information types. in our case they are the \"*observable*\" values from which we want to infer *hidden* values\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**NOTE**: </span>\n",
    "\n",
    "- For *hidden* variables you should only select ``temperature`` then ``data_types_used = [0]``. \n",
    "- For *observale* variables you should, of course, choose the same ones as in the SOM phase! In our case we only used ``sst`` and ``dswr`` but feel free to play with other variables as well - but don't forget to train the SOM and create the Map2d before ! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.2 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# --- Select your data by initializing data_types_used\n",
    "# - 0: Temperature (temp)\n",
    "# - 1: Sea surface temperature (sst)\n",
    "# - 2: Incoming Short Wave Radiation (dswr)\n",
    "# - 3: Air Temperature (airt)\n",
    "# - 4: Wind Speed (ws10)\n",
    "# - (-1): Depth (depth)\n",
    "data_types_used_obs = [1,2]  # suggested observable variables \n",
    "data_types_used_hid = [0]  # hidden variables\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.2 Select the 4 Map2d \n",
    "\n",
    "###### 0.2.1 Reconstruct paths and filenames\n",
    "\n",
    "You need 4 maps to train and test your HMM. \n",
    "- ``map_obs_train``\n",
    "- ``map_obs_test``\n",
    "- ``map_hid_train``\n",
    "- ``map_hid_test``\n",
    "\n",
    "These maps have to be created by 2 different SOM: ``som_obs``Â and ``som_hid`` Their shape determines the shape of the matrices defining the HMM: ``(Tr, Em, pi)`` You should then specify both shapes in order to initialize the HMM.\n",
    "\n",
    "In addition, if you used the defaults path and filenames while creating the maps you have only have to specify ``nb_years``, ``prob_obs`` and ``prob_dis`` if you want to reconstruct all the paths and filenames of the saved objects. Otherwise you can specify their corresponding names and filenames. \n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "- Specify the shapes of both SOM related to the observable and hidden variables ``(n_obs,m_obs)`` and ``(n_hid,m_hid)``\n",
    "- Select the number of years in the training dataset ``nb_years``(the same as in the SOMs)\n",
    "- Specify ``prob_obs`` and ``prob_dis``, the probabilities of getting a value for each instant ``t``\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional) </span>\n",
    "- Specify manually the name and paths of the different saved objects if you did not used the defaults ones while creating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.2.1 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select the shape of the Maps you want to work with\n",
    "# (These maps have to be created beforehand!)\n",
    "n_obs = 4  \n",
    "m_obs = 12\n",
    "n_hid = 5 \n",
    "m_hid = 80  \n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select the number of years used for the training \n",
    "# (should be the same number as the one used for the SOM)\n",
    "nb_years = 12\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select the probability of missing a vector at each instant\n",
    "prob_obs = 1.\n",
    "prob_hid = 1.\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# ---- Retrieve the names of the data type used to reconstruct the default paths\n",
    "data_name_obs = \"\"\n",
    "for i in data_types_used_obs:\n",
    "    data_name_obs = data_name_obs + data_types[i] + \"-\" \n",
    "data_name_obs=data_name_obs[:-1]\n",
    "data_name_hid = \"\"\n",
    "for i in data_types_used_hid:\n",
    "    data_name_hid = data_name_hid + data_types[i] + \"-\" \n",
    "data_name_hid=data_name_hid[:-1]\n",
    "\n",
    "# ---- Reconstruct the default paths\n",
    "path_map_obs = \"objects/Map/\" + data_name_obs + \"/\"+str(n_obs)+\"-\"+str(m_obs)+\"/\" +str(int(prob_obs*100))+'/'\n",
    "path_map_hid = \"objects/Map/\" + data_name_hid + \"/\"+str(n_hid)+\"-\"+str(m_hid)+\"/\"+str(int(prob_hid*100))+'/'\n",
    "\n",
    "# ---- Reconstruct the default filenames\n",
    "name_train = \"1992-\"+str(1992+nb_years-1)+\"_mapped\"    \n",
    "#name_test = str(1992+nb_years)+\"-2007_mapped\"  # for complete testing dataset\n",
    "name_test = \"2007_mapped\"  # to select only one testing year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 0.2.1 Load the maps \n",
    "\n",
    "Once all the names and filenames are initialized you can load your 4 maps! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.2.2 ##################################################\n",
    "# ---- Load the maps with the defaults filenames and paths\n",
    "map_obs_train = map2d.load(path=path_map_obs, filename=name_train)\n",
    "map_obs_test = map2d.load(path=path_map_obs, filename=name_test)\n",
    "map_hid_train = map2d.load(path=path_map_hid, filename=name_train)\n",
    "map_hid_test = map2d.load(path=path_map_hid, filename=name_test)\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"------------------- SOM OBS --------------------\")\n",
    "\n",
    "print(\"TRAINED WITH \", map_obs_train.som.nb_training_iterations, \"iterations \")\n",
    "print(\"INPUT MAPPED: \", map_obs_train.som.nb_inputs_mapped, \"iterations \")\n",
    "\n",
    "\n",
    "print(\"------------------- SOM HID --------------------\")\n",
    "\n",
    "print(\"TRAINED WITH \", map_hid_train.som.nb_training_iterations, \"iterations \")\n",
    "print(\"INPUT MAPPED: \", map_hid_train.som.nb_inputs_mapped, \"iterations \")\n",
    "print(\"MEAN DISTANCE TRANSITION \", np.mean(map_hid_train.som.distance_transitions))\n",
    "print(\"STDEV DISTANCE TRANSITION \", np.std(map_hid_train.som.distance_transitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: HMM configuration and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Initialize Hmm object\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**NOTE**: </span>\n",
    "\n",
    "In order to better visualize and monitor the training, we will use the Hmm model to reconstruct the vertical profiles from ``map_obs_test.classes`` with the ``viterbi`` method at each step of the training. Then - at each step - we will plot the emission and transition matrices ``Em`` and ``Tr`` and the reconstructed vertical profiles.  There are many other ways to visualize and evaluate the results such as plotting the error or comparing to another model output, so do not hesitate to try other options! \n",
    "\n",
    "###### 1.1.1 Define paths and filenames\n",
    "\n",
    "You need 4 maps to train and test your HMM. \n",
    "- ``map_obs_train``\n",
    "- ``map_obs_test``\n",
    "- ``map_hid_train``\n",
    "- ``map_hid_test``\n",
    "\n",
    "These maps have to be created by 2 different SOM: ``som_obs`` and ``som_hid`` Their shape determines the shape of the matrices defining the HMM: ``(Tr, Em, pi)`` You should then specify both shapes in order to initialize the HMM.\n",
    "\n",
    "In addition, if you used the defaults path and filenames while creating the maps you have only have to specify ``nb_years``, ``prob_obs`` and ``prob_dis`` if you want to reconstruct all the paths and filenames of the saved objects. Otherwise you can specify their corresponding names and filenames. \n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional)</span>\n",
    "\n",
    "- Customize the name of the Hmm ``myHMM``\n",
    "- Customize the name of map estimated by the HMM ``map_hid_est``\n",
    "- Customize the default relative path at which ``map_hid_est`` will be saved\n",
    "- Customize the default relative path at which ``myHMM`` will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.1.1 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: (optional)\n",
    "# ---- Customize default paths and names\n",
    "data_name_hmm = data_name_obs + \"_\" + data_name_hid\n",
    "shape_hmm = str(n_obs)+\"-\"+str(m_obs)+\"_\"+str(n_hid)+\"-\"+str(m_hid)\n",
    "path_map_est = \"objects/Map/\" + data_name_hmm + \"/\" + shape_hmm + \"/\" + str(int(prob_hid*100)) +'/'\n",
    "path_hmm = \"objects/Hmm/\" + data_name_hmm + \"/\" + shape_hmm + \"/\" + str(int(prob_hid*100)) +'/'\n",
    "name_est = str(1992+nb_years)+\"-2007_mapped_est\"\n",
    "name_hmm = \"1992-\"+str(1992+nb_years-1)+\"_trained\"\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.1.2 Initialize Hmm object and model\n",
    "\n",
    "- The function ``hmm.Hmm`` creates the Hmm object\n",
    "- The method ``init_model`` initializes the probabilities (elements of the matrices ``(Tr, Em, pi)``) by counting the transitions and emission in the training dataset\n",
    "- pi is not really important and we don't have enough years in our training dataset so our prior is not informative enough. A uniform probability works well though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############ STEP 1.1.2 ##################################################\n",
    "\n",
    "print(\" #################### INITIALIZATION ##########################\")\n",
    "myHMM = hmm.Hmm(n_obs*m_obs,n_hid*m_hid, name=name_hmm)\n",
    "myHMM.init_model(map_obs_train.classes, map_hid_train.classes)\n",
    "myHMM.pi = np.ones((n_hid*m_hid))  \n",
    "\n",
    "print(\" #################### VISUALIZATION ##########################\")\n",
    "classes_hid_est = myHMM.viterbi(map_obs_test.classes)\n",
    "map_hid_est = map2d.Map2d(som=map_hid_test.som, name=name_est)\n",
    "map_hid_est.map_from_classes(classes_hid_est, true_values=map_hid_test.true_values, overwrite=True)\n",
    "\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "sns.heatmap(myHMM.Tr, annot=False)\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "sns.heatmap(myHMM.Em, annot=False)\n",
    "\n",
    "print_data(map_hid_est.values,depth,zmin=18,zmax=32, legend=\"Estimated values\", figsize=(10,4))\n",
    "print_data(map_hid_test.values,depth,zmin=18,zmax=32, legend=\"Expected values\", figsize=(10,4))\n",
    "print_data(map_hid_est.true_values,depth,zmin=18,zmax=32, legend=\"True values\", figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Training\n",
    "\n",
    "###### 1.2.1 Neighborhood function\n",
    "\n",
    "As mentioned in the configuration step of the SOM, the more classes there are, the more difficult it is to estimate the model parameters of the HMM. Here, our training dataset is not large enough to provide the HMM with enough different emissions and transitions: a lot of *possible* emissions and transitions are never observed in the training dataset but they could have been and their probability should not be null. We should then find a way to increase their probability without increasing probability of physically impossible events. \n",
    "\n",
    "One of the main advantages of SOM is that referent vectors associated to topologically neighbouring classes have similar properties. For instance, since we are working with time series referent vectors that are close to each other in the map represent the same period of the year and share common features. \n",
    "\n",
    "Then in our case, the number of classes in higher in ``som_hid`` than in ``som_obs`` because the vertical profiles are more complex than surface data. Thus the PROFHMM method suggests, we use a neighboring function that exploits the topological properties of the SOM to âpropagateâ the probability of a hidden class to its neighbors.\n",
    "\n",
    "$$\n",
    "     Tr(i,j) = S_{Tr}^{-1}\\sum_{k=1}^{N_{\\rm dis}} exp(-\\frac{dist(j,k)}{\\sigma}) Tr(i,k) \n",
    "$$\n",
    "\n",
    "$$\n",
    "    Em(i,j) = S_{Em}^{-1}\\sum_{k=1}^{N_{\\rm dis}} exp(-\\frac{dist(j,k)}{\\sigma}) Em(i,k) \n",
    "$$\n",
    "\n",
    "with $S_{Tr}^{-1}$ and $S_{Em}^{-1}$ normalizing factors such as $\\sum_{j=1}^{N_{dis}} Tr(i,j)= \\sum_{i=1}^{N_{obs}} Em(i,j) = 1$\n",
    "\n",
    "Now we have to choose $\\sigma$ - the typical distance to which the probabilities should be spread.\n",
    "\n",
    "- if $\\sigma$ too is small, the new probabilities will be very close to the original \n",
    "\n",
    "- if $\\sigma$ is too large, the new probabilities will not be almost uniform throughout the hidden map.\n",
    "\n",
    "This typical distance is strongly linked to the nature of the data used and the shape of the SOM, thus using the same radius regardless of the data and the SOM used could be irrelevant. Instead we can compute $\\sigma$ according to the actual typical distance covered by each transition that occurred while training the SOM ``som_hid``\n",
    "\n",
    "In our case we use the mean of the distance covered by transition during the training of ``som_hid`` after removing outliers (are regarded as outliers distance that are superier\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional)</span>\n",
    "- Change the value of ``outlier_threshold`` and observe the results! :) \n",
    "- Change the value of ``sigma`` and observe the results! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############ STEP 1.2.1 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: (optional)\n",
    "# ---- Select a threshold\n",
    "outlier_threshold = 1.\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "ref_distance_transitions = map_hid_train.som.distance_transitions\n",
    "print(\"MEAN DISTANCE OF TRANSITIONS:\", np.mean(ref_distance_transitions))\n",
    "print(\"STDEV DISTANCE OF TRANSITIONS: \", )\n",
    "print(\"THRESHOLD USED: \", outlier_threshold)\n",
    "q = np.quantile(ref_distance_transitions, outlier_threshold)\n",
    "print(\"DISTANCE MAX TRAVELLED BY \",str(outlier_threshold*100),\"% OF TRANSITIONS \", q)\n",
    "ref_distance_transitions = np.where(ref_distance_transitions>q, np.nan, ref_distance_transitions)\n",
    "dist_ref_mean = np.nanmean(ref_distance_transitions)\n",
    "dist_ref_std = np.nanstd(ref_distance_transitions)\n",
    "print(\"MEAN DISTANCE OF TRANSITIONS AFTER REMOVING OUTLIERS: \", dist_ref_mean)\n",
    "print(\"STDEV DISTANCE OF TRANSITIONS AFTER REMOVING OUTLIERS: \", dist_ref_std)\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: (optional)\n",
    "# ---- Change the value of sigma\n",
    "sigma = dist_ref_mean\n",
    "# ----------------------------\n",
    "\n",
    "print(\"SIGMA USED: \", sigma)\n",
    "\n",
    "print(\" #################### NEIGHBORHOOD ##########################\")\n",
    "# Apply the neighborhood function to Tr and Em\n",
    "myHMM.neighborhood(sigma=dist_ref_mean,\n",
    "                    distance_matrix=map_hid_train.som.distance_matrix)\n",
    "\n",
    "print(\" #################### VISUALIZATION ##########################\")\n",
    "classes_hid_est = myHMM.viterbi(map_obs_test.classes)\n",
    "map_hid_est = map2d.Map2d(som=map_hid_test.som, name=name_est)\n",
    "map_hid_est.map_from_classes(classes_hid_est, true_values=map_hid_test.true_values, overwrite=True)\n",
    "\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "sns.heatmap(myHMM.Tr, annot=False)\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "sns.heatmap(myHMM.Em, annot=False)\n",
    "\n",
    "\n",
    "print_data(map_hid_est.values,depth,zmin=18,zmax=32, legend=\"Estimated values\", figsize=(10,4))\n",
    "print_data(map_hid_test.values,depth,zmin=18,zmax=32, legend=\"Expected values\", figsize=(10,4))\n",
    "print_data(map_hid_est.true_values,depth,zmin=18,zmax=32, legend=\"True values\", figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.2.2 Baum-Welch\n",
    "\n",
    "Since we initialized the HMM by counting the number of transitions and emission in the training dataset, we already have a good background information. As the consequence a few iterations only are required by the Baum-Welch algorithm and too many iterations may cause some numerical instabilities. \n",
    "You can change ``iterations`` if you want to see for yourself!\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional)</span>\n",
    "- Change the value of ``iterations`` and observe the results! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############ STEP 1.2.2 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: (optional)\n",
    "# ---- change the number of iterations \n",
    "iterations = 5\n",
    "# ----------------------------\n",
    "\n",
    "print(\" #################### BAUM-WELCH ##########################\")\n",
    "for i in range(iterations):\n",
    "    myHMM.bw(np.concatenate([map_obs_train.classes, map_obs_test.classes]))\n",
    "\n",
    "print(\" #################### VISUALIZATION ##########################\")\n",
    "classes_hid_est = myHMM.viterbi(map_obs_test.classes)\n",
    "map_hid_est = map2d.Map2d(som=map_hid_test.som, name=name_est)\n",
    "map_hid_est.map_from_classes(classes_hid_est, true_values=map_hid_test.true_values, overwrite=True)\n",
    "\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "sns.heatmap(myHMM.Tr, annot=False)\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "sns.heatmap(myHMM.Em, annot=False)\n",
    "\n",
    "\n",
    "print_data(map_hid_est.values,depth,zmin=18,zmax=32, legend=\"Estimated values\", figsize=(10,4))\n",
    "print_data(map_hid_test.values,depth,zmin=18,zmax=32, legend=\"Expected values\", figsize=(10,4))\n",
    "print_data(map_hid_est.true_values,depth,zmin=18,zmax=32, legend=\"True values\", figsize=(10,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3 Smooth transitions\n",
    "\n",
    "Baum-Welch is an expectation-maximization algorithm that only takes as input the observable time-series. If the training dataset is too small this might cause some discontinuities in the hidden time-series reconstructed. \n",
    "\n",
    "However, as mentioned in the step 1.2.1, to deal with this kind of problem we can use a neighborhood function that exploits SOM topological properties. Nevertheless, the objective is not to spread the probability of a hidden class to its neighbors but to decrease the probability of transiting from $i$ to $j$ if $i$ and $j$ are far from each other in ``Ìsom_hid`` \n",
    "\n",
    "We use then another method called ``smooth_transitions`` that updates the transition matrix as follows:\n",
    "\n",
    "$$\n",
    "Tr(i,j) =S^{-1} exp(-\\frac{dist(i,j)}{\\sigma}) Tr(i,j) \n",
    "$$\n",
    "\n",
    "with $S_{Tr}^{-1}$ a normalizing factor such as $\\sum_{j=1}^{N_{dis}} Tr(i,j) = 1$\n",
    "\n",
    "- if $\\sigma$ too is small, the model will barely transit from one class to another\n",
    "\n",
    "- if $\\sigma$ is too large, the new probabilities will be almost the same as before\n",
    "\n",
    "We use here the same radius $\\sigma$ as in the other neighbourhood function for the same reasons but once again, feel free to try other radius! \n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional)</span>\n",
    "- Change the value of ``sigma`` and observe the results! :) \n",
    "- Change the condition of applying the neighborhood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############ STEP 1.2.3 ##################################################\n",
    "\n",
    "dist_mean = np.mean(map_hid_est.distance_transitions)\n",
    "dist_std = np.std(map_hid_est.distance_transitions)\n",
    "if (dist_mean>dist_ref_mean):\n",
    "    print(\" #################### SMOOTH_TRANSITIONS ##########################\")\n",
    "    dist_matrix = map_hid_est.som.distance_matrix\n",
    "    myHMM.neighborhood(sigma=dist_ref_mean,\n",
    "                        distance_matrix=map_hid_train.som.distance_matrix)\n",
    "\n",
    "    classes_hid_est = myHMM.viterbi(map_obs_test.classes)\n",
    "    map_hid_est.map_from_classes(classes_hid_est, true_values=map_hid_test.true_values, overwrite=True)\n",
    "    \n",
    "print(\" #################### VISUALIZATION ##########################\")\n",
    "classes_hid_est = myHMM.viterbi(map_obs_test.classes)\n",
    "map_hid_est = map2d.Map2d(som=map_hid_test.som, name=name_est)\n",
    "map_hid_est.map_from_classes(classes_hid_est, true_values=map_hid_test.true_values, overwrite=True)\n",
    "\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "sns.heatmap(myHMM.Tr, annot=False)\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "sns.heatmap(myHMM.Em, annot=False)\n",
    "\n",
    "\n",
    "print_data(map_hid_est.values,depth,zmin=18,zmax=32, legend=\"Estimated values\", figsize=(10,4))\n",
    "print_data(map_hid_test.values,depth,zmin=18,zmax=32, legend=\"Expected values\", figsize=(10,4))\n",
    "print_data(map_hid_est.true_values,depth,zmin=18,zmax=32, legend=\"True values\", figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: HMM results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = map2d.confusion_matrix(map_hid_test, map_hid_est)\n",
    "fig= plt.figure(figsize=(15,15))\n",
    "sns.heatmap(confusion_matrix, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_between_labels = map2d.distance_between_labels(map_hid_test, map_hid_est)\n",
    "print(distance_between_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = map_hid_est.true_values - map_hid_est.values\n",
    "zmax=np.max(np.abs(difference))\n",
    "print(\"ABSOLUTE ERROR MAX: \", zmax) \n",
    "print(\"MEAN ABSOLUTE ERROR: \", np.mean(np.abs(difference)))\n",
    "\n",
    "print_data(difference,depth,zmin=-zmax,zmax=zmax, legend=\"Difference\", figsize=(10,4), cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are satisfied with the results you can save your Hmm and Map2d objects! :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2d.save(map_hid_est, path=path_map_est)\n",
    "hmm.save(myHMM, path=path_hmm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
