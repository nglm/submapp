{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import seaborn as sns\n",
    "from submapp import *\n",
    "from tools.tools_som import *\n",
    "from submapp.tools.data_processing import *\n",
    "from submapp.tools.evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.1 Extract data from files\n",
    "\n",
    "There is nothing to change in this cell, just run the cell to extract data from the netcdf4 file. \n",
    "If this does not work, make sure the file ``GotmFabmErsem-BATS.nc`` is in the right folder. (It should be in the same folder as this notebook, ``submapp`` folder and ``tools`` folder )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.1 ##################################################\n",
    "# ---- Extracting data\n",
    "([temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth]) = data_extraction()\n",
    "gotm_data = [temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth]\n",
    "data_types = [\"temp\", \"sst\", \"dswr\", \"airt\", \"ws10\", \"depth\"]\n",
    "\n",
    "for i in range(len(gotm_data)-1):\n",
    "    for y in range(len(gotm_data[i])):\n",
    "        gotm_data[i][y] = mean_steps(gotm_data[i][y],10)\n",
    "[temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth] = gotm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.2 Select data types\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Select the data types you want to work with by initializing the list ``data_types_used`` with some of the following elements \n",
    "     - 0: Temperature (temp)\n",
    "     - 1: Sea surface temperature (sst)\n",
    "     - 2: Incoming Short Wave Radiation (dswr)\n",
    "     - 3: Air Temperature (airt)\n",
    "     - 4: Wind Speed (ws10)\n",
    "     - (-1): Depth (depth)\n",
    "     \n",
    "``Temperature (0)`` is related to the vertical profiles of temperature, in our case they are the \"*hidden*\" values.\n",
    "\n",
    "`` Depth (-1)`` is a vector of levels of depth corresponding to the vertical profiles\n",
    "\n",
    "``Sea surface temperature (1), Incoming Short Wave Radiation (2), Air Temperature (3)`` and ``Wind Speed (4)`` are surface information types. in our case they are the \"*observable*\" values from which we want to infer *hidden* values\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**NOTE**: </span>\n",
    "\n",
    "- If you want to work with *hidden* variables then you should only select ``temperature`` then ``data_types_used = [0]``. \n",
    "- If you want to work with *observale* variables you are not forced to choose all the observable variables! Actually you should NOT use them all as some of them are highly correlated! In our case we will only use ``sst`` and ``dswr`` but feel free to play with other variables as well! :) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.2 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# --- Select your data by initializing data_types_used\n",
    "# - 0: Temperature (temp)\n",
    "# - 1: Sea surface temperature (sst)\n",
    "# - 2: Incoming Short Wave Radiation (dswr)\n",
    "# - 3: Air Temperature (airt)\n",
    "# - 4: Wind Speed (ws10)\n",
    "# - (-1): Depth (depth)\n",
    "data_types_used = [1,2]  # suggested observable variables \n",
    "# data_types_used = [0]  # hidden variables\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.3 Split dataset\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Select the number of years in the training dataset (between ``0`` and ``16`` in our case). We suggest you choose ``12``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.3 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select the number of years in the training dataset (SUGGESTED 12)\n",
    "nb_years_train = 12\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# Quick summary -- nothing to do here\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "if 0 in data_types_used:\n",
    "    surface=False\n",
    "    data_y = [gotm_data[0]]\n",
    "    nb_data_types_used = 1\n",
    "    p = depth.size\n",
    "else:\n",
    "    surface=True\n",
    "    nb_data_types_used = len(data_types_used)\n",
    "    data_y = [gotm_data[i] for i in data_types_used]\n",
    "    p = nb_data_types_used\n",
    "    \n",
    "\n",
    "print(\"DATA TYPE USED : \", [data_types[i] for i in data_types_used])\n",
    "print(\"ARE DATA OBSERVABLE: \", surface)\n",
    "\n",
    "nb_years = len(data_y[0])\n",
    "print(\"SIZE OF INPUT VECTORS :\", p)\n",
    "print(\"NUMBER OF YEARS USED FOR TRAINING: \", nb_years_train)\n",
    "print(\"NUMBER OF YEARS USED FOR TESTING: \", nb_years-nb_years_train)\n",
    "print(\"TOTAL NUMBER OF YEARS: \", nb_years)\n",
    "\n",
    "T_y=[]\n",
    "for y in range(nb_years):\n",
    "    T = len(data_y[0][y])\n",
    "    T_y = np.concatenate([T_y, [T]])\n",
    "    print(\"YEAR \", str(1992+y), \" LENGTH : \", T )\n",
    "T_y = T_y.astype(int)\n",
    "print(\"LENGTH OF THE TRAINING TIME SERIES: \", sum(T_y[:nb_years_train]))\n",
    "print(\"LENGTH OF THE TESTING TIME SERIES: \", sum(T_y[nb_years_train:]))\n",
    "print(\"TOTAL LENGTH OF THE TIME SERIES : \", sum(T_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.3.bis [OPT] Emulate missing values\n",
    "\n",
    "If you want to emulate missing values in your datasets you can specify a probability lower than 1 of getting an element for all instant of the time series.\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Specify ``prob``: define the probability ``p`` ( with ``0 <= p <= 1``) of getting a value for each instant ``t``\n",
    "\n",
    "- Specify if this probability should be applied for the training phase set and/or the mapping phase:\n",
    "    - ``complete_data_train = True`` if you want to use the complete dataset for the training phase\n",
    "    - ``complete_data_map = False`` if you want to use the sparse dataset for the mapping phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.3.bis ###############################################\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select the probability of getting data at each instant t\n",
    "prob = 1.\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select if the complete dataset should be used for training/mapping\n",
    "# ---- Or only the sparse dataset\n",
    "complete_data_train = True\n",
    "complete_data_map = False\n",
    "# ----------------------------\n",
    "\n",
    "# --- Remove data randomly\n",
    "complete_data = data_y\n",
    "\n",
    "sparse_data = [[]]*nb_data_types_used\n",
    "for dtype in range(nb_data_types_used):\n",
    "    sparse_data[dtype] = [remove_random_data(data, prob=prob) for data in data_y[dtype]]\n",
    "\n",
    "if complete_data_train:\n",
    "    data_train = complete_data\n",
    "else:\n",
    "    data_train = sparse_data\n",
    "if complete_data_map:\n",
    "    data_map = complete_data\n",
    "else:\n",
    "    data_map = sparse_data\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.4 Standardization\n",
    "\n",
    "There is nothing to do here, run the cell to standardize the data and have a quick summary about the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.4 ###############################################\n",
    "\n",
    "# --- Standardization: step 1: find standardization coeff (on the training data set only!)\n",
    "data_train_tot = [np.concatenate(data[:nb_years_train]) for data in data_train]\n",
    "data_mean = np.zeros(nb_data_types_used)\n",
    "data_stdev = np.zeros(nb_data_types_used)\n",
    "data_max = np.zeros(nb_data_types_used)\n",
    "data_min = np.zeros(nb_data_types_used)\n",
    "for i in range(nb_data_types_used):\n",
    "    data_max[i] = np.max(data_train_tot[i])\n",
    "    data_min[i] = np.min(data_train_tot[i])\n",
    "    (_ ,data_mean[i], data_stdev[i]) = standardize(data_train_tot[i])\n",
    "\n",
    "# --- standardization: step 2: standardize\n",
    "data_train_norm = [np.empty([]) for y in range(nb_years)]\n",
    "data_map_norm = [np.empty([]) for y in range(nb_years)]\n",
    "for y in range(nb_years):\n",
    "    if len(data_y[0][0].shape) == 1:\n",
    "        p_tmp=1\n",
    "    else:\n",
    "        (_,p_tmp) = data_y[0][0].shape\n",
    "    data_train_norm[y] = np.concatenate([\n",
    "        np.reshape(standardize(data_train[i][y], \n",
    "                  data_mean[i], \n",
    "                  data_stdev[i]),(T_y[y],p_tmp) ) for i in range(nb_data_types_used)], axis=1)\n",
    "    data_map_norm[y] = np.concatenate([\n",
    "        np.reshape(standardize(data_map[i][y], \n",
    "                  data_mean[i], \n",
    "                  data_stdev[i]),(T_y[y],p_tmp) ) for i in range(nb_data_types_used)], axis=1)\n",
    "    \n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "for i in range(nb_data_types_used):\n",
    "    print(\"DATA TYPE: \", \n",
    "          data_types[data_types_used[i]],\n",
    "          \" MEAN: \", round(data_mean[i],2),\n",
    "          \" STDEV: \", round(data_stdev[i],2),\n",
    "          \" MIN: \", round(data_min[i],2),\n",
    "          \" MAX: \", round(data_max[i],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: SOM configuration and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Choose the shape of the SOM\n",
    "\n",
    "__Shape (n, m)__\n",
    "\n",
    "By *shape of the SOM* we mean here its number of row ``n`` and its number of column ``m``.\n",
    "\n",
    "It is one of the most important parameters and we highly recommend you to test a lot of different shapes. To help you decide on the ratio ``m/n`` you can use the function ``pca_features`` whose first output is the ratio between the first 2 principal components. This ratio can be seen as a suggestion and the second output (the cumulative variance of the first 2 principal components) can be seen as an indicator of relevance of this suggestion. Once again, all this is only about \"*suggestion*\", \"*indication*\" and there is no garantee that this will give the best shape nor even a good one. Once again we highly recommend you to try different shapes!\n",
    "\n",
    "__Total number of classes n*m__\n",
    "\n",
    "The total number of classes ``nb_class=n*m`` matters as well. If this number is too low the SOM will poorly represent the diversity of the data. If this number is too high the SOM may overfit the training data and significantly loose accuracy when it comes to the testing dataset. This is a very commom problem in machine learning \n",
    "and there are tons of articles dealing with this issue. If you want to find out more about overfitting, how to detect it and prevent it check out [this article](https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42) by Julien Despois from which the 2 following figures are:\n",
    "<figure>\n",
    "<img src=\"img/overfitting01.png\" alt=\"Recognize underfitting and overfitting\" width=\"600\" />\n",
    "<figcaption> <center> Fig.1 - Recognize underfitting and overfitting </center> </figcaption>\n",
    "<img src=\"img/overfitting02.png\" alt=\"Overfitting and model complexity\" width=\"500\"/>\n",
    "<figcaption> <center> Fig.2 - Overfitting and model complexity </center> </figcaption>\n",
    "</figure>\n",
    "\n",
    "However, in our context, both SOM we trained are to be combined with a HMM. In this situation, the bottom line is the accuracy of the prediction of hidden variables given observable variables and not the accuracy of the SOMs themselves. Another problem here is that the more classes there are, the more difficult it is to estimate the model parameters of the HMM, particularly the transition matrix ``Tr``. Thus even though the SOM are not overfitting the training dataset we may have to train new SOMs with fewer classes according to the output of the HMM. \n",
    "\n",
    "The accuracy of the hidden SOM is important though since it defines the best accuracy the HMM can reach. Indeed, in the ideal case your HMM estimates the best classses possible therefore the only error made is the difference between the *true value* and the referent vector representing this *true value*\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Specify ``n`` the number of rows and ``m`` the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.1 ###############################################\n",
    "\n",
    "#  Apply PCA on the training dataset. \n",
    "# - r: ratio between the first 2 principal components. \n",
    "# - c_v: cumulative variance of the first 2 principal components. \n",
    "r, c_v = pca_features(remove_nan(np.concatenate(data_train_norm[:nb_years_train])))\n",
    "\n",
    "# ----------------------------\n",
    "# TODO:\n",
    "# ---- Select the number of class and the shape of the SOM\n",
    "# For instance (n,m) = (4,70) for hidden variables and (n,m) = (4,10) for observable variables\n",
    "n = 4\n",
    "m = 70\n",
    "# ----------------------------\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"RATIO PC1/PC2: \", round(r,2), \"\\nCUMULATIVE VARIANCE: \", c_v)\n",
    "print(\"SHAPE OF THE SOM :  (\", n, \"x\", m, \")\")\n",
    "print(\"RATIO m/n:  \", round(m/n,2))\n",
    "print(\"TOTAL NUMBER OF CLASSES: \", n*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 SOM initialization\n",
    "\n",
    "###### 1.2.1 SOM name and path\n",
    "\n",
    "If you want to save your ``Som`` object you should customize the name your ``Som`` object and the relative paths at which the objects have to be saved by default to make it easier to load them afterwards. \n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional)</span>\n",
    "\n",
    "- Customize the name of the SOM\n",
    "- Customize the default relative path at which the Som object will be saved\n",
    "- Customize the default relative path at which the figures will be saved\n",
    "- Customize the default relative path at which the Map2d object will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.2 ###############################################\n",
    "\n",
    "############ STEP 1.2.1 ###############################################\n",
    "\n",
    "# Retrieve the name of the data type used to customize the path\n",
    "data_name = \"\"\n",
    "for i in data_types_used:\n",
    "    data_name = data_name + data_types[i] + \"-\" \n",
    "data_name=data_name[:-1]\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: (Optional)\n",
    "# ---- Customize the different default paths\n",
    "path_fig = \"figs/Map/\" + data_name + \"/\" + str(n) + \"-\" + str(m) +\"/\" + str(int(prob*100)) +'/'\n",
    "path_som = \"objects/Som/\" + data_name + \"/\" + str(n) + \"-\" + str(m) +\"/\" + str(int(prob*100)) +'/'\n",
    "path_map = \"objects/Map/\" + data_name + \"/\" + str(n) + \"-\" + str(m) +\"/\" + str(int(prob*100)) +'/'\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: (Optional)\n",
    "# ---- Customize the name of the Som object\n",
    "name=\"1992-\"+str(1992+nb_years_train-1)+\"_trained\"\n",
    "# ----------------------------\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"NAME OF THE SOM: \", name )\n",
    "print(\"FIGS DIRECTORY: \",path_fig)\n",
    "print(\"SOM DIRECTORY: \",path_som)\n",
    "print(\"MAP DIRECTORY: \",path_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.2.2  SOM and weights initialization\n",
    "\n",
    "Once the shape of the SOM is chosen, we can initialize our object ``mySom`` from the class ``Som``.\n",
    "\n",
    "Then weights have to be initialized. The initialization may influence:\n",
    "- The learning speed: how many iterations are necessary before getting good results\n",
    "- The organization of specialized areas within the SOM: where are the neurons representing the spring, the cool down, etc\n",
    "- Final performance (after the training)\n",
    "\n",
    "The learning rate and the radius of the neighborhood function can limit this influence. Indeed if both initial learning rate and radius are high, the very first inputs will re-organize the whole SOM almost independently of the initial state of the SOM. However a good initialization may ensure a stable and coherent internal structure of the SOM.\n",
    "\n",
    "\n",
    "Weights can be initialized in many ways and none of them is better than all others so you can try different initialization processes. \n",
    "\n",
    "There are mainly 3 different approaches:\n",
    "- Random initialization: each weight is a sample drawn from a distribution (Gaussian or a uniform for example)\n",
    "- Sample initialization: each weight is a sample drawn from the training dataset\n",
    "- Data analysis based initialization: each weight is spanned by eigenvectors of the input dataset or a referent vector of a K-means method, etc.\n",
    "\n",
    "Here you can use one of these methods\n",
    "- ``random_weights_initialization``: Initializes weights randomly with a gaussian or uniform distribution\n",
    "- ``pca_weights_initialization``: Initializes weights by applying a PCA on athe training dataset\n",
    "- ``sample_weights_initialization``: Initializes weights with a sample of the training data\n",
    "- ``zeros_weights_initialization``: Initializes all weights to zeros vectors\n",
    "\n",
    "Otherwise you can specify your own weights when initializing the Som object\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Initialize the weights\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional) </span>\n",
    "- Try different initialization and observe the differences by runnning the next cell (step 1.2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.2.2 ###############################################\n",
    "\n",
    "# Initialize the Som object \n",
    "mySom = som.Som(n,m,p,weights=None,name=name,data_mean=data_mean,data_stdev=data_stdev)\n",
    "\n",
    "# ----------------------------\n",
    "# TODO:\n",
    "# ---- Initialize the weights with one of these methods \n",
    "# ---- (or None of them but in that case specify your own weights when initializing the Som object)\n",
    "mySom.random_weights_initialization(distribution=\"gaussian\")\n",
    "mySom.pca_weights_initialization(np.concatenate(data_train_norm[:nb_years_train]))\n",
    "mySom.sample_weights_initialization(np.concatenate(data_train_norm[:nb_years_train]))\n",
    "# mySom.zeros_weights_initialization()\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.2.3 Weights visualization\n",
    "\n",
    "Now that our weights are initiliazed we can try to visualize them\n",
    "\n",
    "These plots can help you understand how weights initialization works but also how hard it is to interpret and visualize weights! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.2.3 ###############################################\n",
    "\n",
    "if not surface or p>1:\n",
    "    # If we are using observable variables, each element of the vector represents a different type of data\n",
    "    # and should be treated separately\n",
    "    if surface:\n",
    "        for i in range(nb_data_types_used):\n",
    "            print(\"DATA TYPE : \", data_types[data_types_used[i]])\n",
    "            print(\"VALUE OF EVERY WEIGHT OF THE SOM \")\n",
    "            weights_mean = mySom.weights_features(features=\"mean\",start=i,end=i+1)\n",
    "            mySom.print_heatmap(weights_mean)\n",
    "    # If we are using hidden variables, each element of the vector represents a different level of depth \n",
    "    # of the temperature vertical profile and we can \"average\" them to make the plotting easier to interpret \n",
    "    else:\n",
    "        print(\"TEMPERATURE, AVERAGE VALUE (ALL LEVELS OF DEPTH) OF EVERY WEIGHT OF THE SOM \")\n",
    "        weights_mean = mySom.weights_features(features=\"mean\")\n",
    "        mySom.print_heatmap(weights_mean)\n",
    "        print(\"TEMPERATURE, STANDARD DEVIATION (ALL LEVELS OF DEPTH) OF EVERY WEIGHT OF THE SOM \")\n",
    "        weights_dev = mySom.weights_features(\"std\")\n",
    "        mySom.print_heatmap(weights_dev)\n",
    "        parts = 4  # the whole profiles will be decomposed into 4 smaller profiles \n",
    "        for i in range(parts):\n",
    "            start=i*p//parts\n",
    "            end=(i+1)*p//parts\n",
    "            print(\"TEMPERATURE, AVERAGE VALUE(LEVELS OF DEPTH\", p-start, \"TO\", p-end,\")\" )\n",
    "            weights_mean = mySom.weights_features(features=\"mean\",start=start,end=end)\n",
    "            mySom.print_heatmap(weights_mean)\n",
    "            print(\"TEMPERATURE, STANDARD DEVIATION (LEVELS OF DEPTH\", p-start, \"TO\", p-end,\")\" )\n",
    "            weights_std = mySom.weights_features(features=\"std\",start=start,end=end)\n",
    "            mySom.print_heatmap(weights_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 SOM training\n",
    "\n",
    "Here a quick reminder about the training phase. \n",
    "\n",
    "```\n",
    "1. Find the Best Matching Unit (BMU): class whose referent vector is the closest to the input\n",
    "2. Update every referent vector: \n",
    "    1. compute the distance between the class associated with the referent vector and the BMU (in the map)\n",
    "    2. update the referent vector so that the closer their associated class is to the BMU (in the map) the more their referent vector will be modified towards the input data.\n",
    "```\n",
    "\n",
    "Thus - for instance - for each instant $t$, given the input vector $x_t$ and knowing that the winning neuron is $c$ the weight $w_{i,j}$ is updated as follows:\n",
    "\n",
    "$$w_{i,j} = w_{i,j} + \\alpha \\times h(w_{i,j},c) \\times [w_{i,j}-x_t] $$\n",
    "\n",
    "with:\n",
    "$$ h(w_{i,j},c) = exp(\\frac{dist(w_{i,j},c)}{ \\sigma^{2} } )$$\n",
    "\n",
    "$$ dist(w_{i_1,j_1},w_{i_2,j_2}) = ||i_1-i_2|| + ||j_1-j_2||$$\n",
    "\n",
    "###### 1.3.1 Choose hyperparameters \n",
    "\n",
    "Before training our SOM we will have to choose (a lot of) hyperparameters. \n",
    "Here a quick description of some of them:\n",
    "\n",
    "- **Learning rate** $\\alpha$: ($0<\\alpha<1$)\n",
    "\n",
    "Determine the *rate* (or *speed*) of learning. \n",
    "\n",
    "If $\\alpha$ is too high, the SOM won't be adaptative but will only reproduce its last training inputs. \n",
    "\n",
    "If $\\alpha$ is too low, the SOM won't learn fast enough and the weights won't be representative enough of the data at the end of the training. \n",
    "\n",
    "The learning rate should decrease over the training. In our case $\\alpha$ decreases linearly after each input vector from ``a0`` to ``aT`` after ``T_train`` inputs but it could have been a exponential decay of an inverse decay as well.\n",
    "\n",
    "- **Radius** $\\sigma$:  ($0<\\sigma$)\n",
    "\n",
    "Determine the $radius$ of the neighborhood function $h$. The neighborhood function determine how much neurons should learn around the *BMU* (Best Matching Unit: the neuron that was the closest to the input vector). \n",
    "\n",
    "If $\\sigma$ is too high, too many neurons will be specialized (or activated) by a given input. As a consequence some neurons might get redundant and therefore useless while some features of the input data will be poorly represented.\n",
    "\n",
    "If $\\sigma$ is too low, the different areas will specialize too slowly. It could have different unwanted consequences: \n",
    "- Multiple small areas corresponding to similar data features instead of a bigger one\n",
    "- Undertraining: some neurons are not trained enough because they are always too far from the *BMU* with respect to the radius.\n",
    "\n",
    "The radius rate should decrease over the training. In our case $\\sigma$ decreases linearly after each input vector from ``s0`` to ``sT`` after ``T_train`` inputs but it could have been a exponential decay of an inverse decay as well.\n",
    "\n",
    "\n",
    "- **Epochs**:\n",
    "\n",
    "Determine the number times that the entire training dataset will train the SOM. \n",
    "\n",
    "During the training phase or once the training is over, plots showing the error with respect to the epochs (sometimes called learning curves) can help to detect overfitting or underfitting. However if the learning rate and the radius decrease enough over the training and if the model is not too complex (total number of classes too high) there should not be overtraining due to a number of epochs too high. \n",
    "\n",
    "- **But also...**\n",
    "\n",
    "There are more hyperparameters such as the distance function $dist(w_{i_1,j_1},w_{i_2,j_2})$ (parameter ``distance_matrix`` while initializing the Som) which determines whether the SOM is a rectangular or an hexagonal map for instance or the cost function that determines the meaning of \"Best Matching Unit\". However, they are beyond the scope of this tutorial and the idea behind the training remains the same. Here we use a rectangular map with the euclidean norm as a cost function.\n",
    "\n",
    "###### 1.3.2 Train\n",
    "\n",
    "Once all the hyperparameters are initialized we can start the training. \n",
    "To do so we use the method ``train`` that updates the weights of the SOM according to the formula above. We repeat the operation ``epochs`` times over the entire training dataset.\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**:</span>\n",
    "\n",
    "- Initialize the number of epochs ``epochs``\n",
    "- Initialize the hyperparameters ``(a0,aT,s0,sT)``\n",
    "- Customize how the learning rate and the radius both decrease over the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ STEP 1.3 ###############################################\n",
    "\n",
    "############ STEP 1.3.1 #############################################\n",
    "# ----------------------------\n",
    "# TODO:\n",
    "# ---- Initialize hyperparameters\n",
    "epochs = 20  # number of epochs\n",
    "a0 = 0.9  # initial learning rate\n",
    "aT = 0.5  # final learning rate (after the first epoch)\n",
    "if surface:\n",
    "    s0 = 4.  # initial radius (if observable variables)\n",
    "    sT = 1  # final radius (if observable variables)\n",
    "else:\n",
    "    s0 = 4.  # initial radius (if hidden variables)\n",
    "    sT = 1  # final radius (if hidden variables)\n",
    "# ----------------------------\n",
    "\n",
    "param = (a0,aT,s0,sT)\n",
    "T_train = len(np.concatenate(data_train_norm[:nb_years_train]))\n",
    "\n",
    "############ STEP 1.3.2 #############################################\n",
    "for e in range(epochs):\n",
    "    print(\"#### Epoch : \", e, \" ####\")\n",
    "    # A random year is selected from the learning dataset in order not to favor one year over another \n",
    "    for y in random.sample(range(nb_years_train), nb_years_train):\n",
    "        # Train the SOM with one more year from the training dataset \n",
    "        inputs = data_train_norm[y]\n",
    "        mySom.train(\n",
    "            data_train=inputs, \n",
    "            param=param, \n",
    "            T_train=T_train\n",
    "        )\n",
    "        # Map the whole testing dataset to evaluate how the SOM has improved since the last training\n",
    "        inputs = np.concatenate(data_map_norm[nb_years_train:])\n",
    "        mySom.map(data_map=inputs)\n",
    "    # ----------------------------\n",
    "    # TODO: \n",
    "    # ---- Hyperparameters can be updated after each epoch for a better control over the training\n",
    "    a0=aT\n",
    "    aT=max(aT/1.2,0.01)\n",
    "    s0=sT\n",
    "    sT=max(sT/1.2,0.5)\n",
    "    # ----------------------------\n",
    "    param = (a0,aT,s0,sT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4 SOM evaluation\n",
    "\n",
    "Model evaluation corresponds to the question \"Are results satisfying?\". This is a very hard question and even more in our case in which we combine 2 machine learning methods. Indeed, we remind you that at the end the performance is determined by the estimation of hidden variables given observable variables and not by the output of the SOMs themselves.\n",
    "\n",
    "Besides, one of the objectives of combining SOM and HMM is to use neighboring functions that propagate probabilities or penalize long transitions. However using these functions is relevant only if the internal structure of both SOM represent well the topology of the data. As a consequence in our situation, making sure that the of the different areas within the SOM are consistent is of paramount importance. Indeed, given a total number of classes, many different sets of parameters will train SOMs with similar accuracies but only few of them will have a consistent enough internal structure for applying neighborhood functions at the HMM stage. \n",
    "\n",
    "\n",
    "\n",
    "First we will try to ask more specific questions:\n",
    "- Is there a problem of overfitting/underfitting?\n",
    "- Is there a problem of overtraining/undertraining?\n",
    "- Are similar weights close to each other - resulting in specialized areas in the map - as they should in a SOM?\n",
    "- Are some neurons redundant?\n",
    "- Are some areas used too often while some are never used for mapping?\n",
    "- Are transitions between the BMU at instant ``t`` and ``t-1`` are mostly short/long?\n",
    "- Is the standard deviation of the distance of transiton low/high?\n",
    "- Are outputs of the SOM representative of the inputs?\n",
    "- Are some periods of year well represented while some others are poorly represented?\n",
    "- Can the SOM represent well enough extreme inputs (very high/low temperature? High on the surface and low in the deep subsurface? \n",
    "\n",
    "Now that the questions are more specific we can try to find some tools that may answer these questions.\n",
    "\n",
    "###### 1.4.1 Learning curves\n",
    "\n",
    "These plots may give an answer to the following questions:\n",
    "\n",
    "- Is there a problem of overfitting/underfitting?\n",
    "- Is there a problem of overtraining/undertraining?\n",
    "\n",
    "We have already mentioned learning curves while defining the hyperparameter ``epochs``. These curves show the error on both training and testing datasets with respect to the number of training iterations. Here are some cases and possible explanation: \n",
    "\n",
    "- The error starts to increase on the testing dataset while it is still decreasing on the training dataset: it is probably due to a problem of overfitting (model too complex? learning rate too high? training dataset not representative?)\n",
    "\n",
    "- Both errors are still decreasing at the end of the training phase: it probably means that the model is undertrained (learning rate too low? epoch too low? radius too low?)\n",
    "\n",
    "- Both errors are constant before the end of the training: The model has reached its optimum performance given its complexity before the specified number of epoch? The learning rate is so low that the updating phase has almost no impact on the weights?\n",
    "\n",
    "- Training error should be lower than the testing one!\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**:</span>\n",
    "- From now on there is nothing to do but interpreting plots and specifying whether objects and figures should be saved or not. But of course feel free to create new critera or plots! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.4.1 #############################################\n",
    "save = False  # Should we save the following figs? \n",
    "filename_train = mySom.name + \"_err_train\"   # Names of the files of the saved figs\n",
    "filename_test = mySom.name + \"_err_test\"\n",
    "\n",
    "print_error(mySom.relerr_train,legend=\"Learning curve: Train dataset\",save=save, path=path_fig, filename=filename_train)\n",
    "print_error(mySom.relerr_test,legend=\"Learning curve: Test dataset\",save=save, path=path_fig, filename=filename_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.4.2 Weights features\n",
    "\n",
    "Here we will plot some features of the weights of the SOM and try to answer some of the previous questions:\n",
    "\n",
    "__Question:__\n",
    "Are similar weights close to each other - resulting in specialized areas in the map - as they should in a SOM?\n",
    "- If there are some discontinuities from one weight to its neighbors then the internal structure of the SOM is really bad. It might be caused by:\n",
    "    - A problem with the neighborhood function: radius too low? Wrong initialization of``distance_matrix`` (defines the distance between classes)?\n",
    "    - Learning rate extremely low so that the training does not really updates the weights?\n",
    "\n",
    "__Question:__\n",
    "Are some neurons redundant?\n",
    "\n",
    "\n",
    "__Question:__\n",
    "Are some periods of year well represented while some others are poorly represented?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mySom.clear_map_info()\n",
    "\n",
    "if not surface or p>1:\n",
    "    # If we are using observable variables, each element of the vector represents a different type of data\n",
    "    # and should be treated separately\n",
    "    if surface:\n",
    "        for i in range(nb_data_types_used):\n",
    "            print(\"DATA TYPE : \", data_types[data_types_used[i]])\n",
    "            print(\"VALUE OF EVERY WEIGHT OF THE SOM \")\n",
    "            weights_mean = mySom.weights_features(features=\"mean\",start=i,end=i+1)\n",
    "            mySom.print_heatmap(weights_mean)\n",
    "            \n",
    "    # If we are using hidden variables, each element of the vector represents a different level of depth \n",
    "    # of the temperature vertical profile and we can \"average\" them to make the plotting easier to interpret \n",
    "    else:\n",
    "        print(\"TEMPERATURE, AVERAGE VALUE (ALL LEVELS OF DEPTH) OF EVERY WEIGHT OF THE SOM \")\n",
    "        weights_mean = mySom.weights_features(features=\"mean\")\n",
    "        mySom.print_heatmap(weights_mean)\n",
    "        print(\"TEMPERATURE, STANDARD DEVIATION (ALL LEVELS OF DEPTH) OF EVERY WEIGHT OF THE SOM \")\n",
    "        weights_dev = mySom.weights_features(\"std\")\n",
    "        mySom.print_heatmap(weights_dev)\n",
    "        parts = 4  # the whole profiles will be decomposed into 4 smaller profiles \n",
    "        for i in range(parts):\n",
    "            start=i*p//parts\n",
    "            end=(i+1)*p//parts\n",
    "            print(\"TEMPERATURE, AVERAGE VALUE(LEVELS OF DEPTH\", p-start, \"TO\", p-end,\")\" )\n",
    "            weights_mean = mySom.weights_features(features=\"mean\",start=start,end=end)\n",
    "            mySom.print_heatmap(weights_mean)\n",
    "            print(\"TEMPERATURE, STANDARD DEVIATION (LEVELS OF DEPTH\", p-start, \"TO\", p-end,\")\" )\n",
    "            weights_std = mySom.weights_features(features=\"std\",start=start,end=end)\n",
    "            mySom.print_heatmap(weights_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.4.2 SOM outputs\n",
    "\n",
    "Here are the outputs of our SOM for the complete dataset. The following plots should help to answer these questions:\n",
    "- Is there a problem of overfitting/underfitting?\n",
    "- Are outputs of the SOM representative of the inputs?\n",
    "- Are some periods of year well represented while some others are poorly represented?\n",
    "- Can the SOM represent well enough extreme inputs (very high/low temperature? High on the surface and low in the deep subsurface? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save=False\n",
    "mySom.clear_map_info()\n",
    "\n",
    "for y in range(0,nb_years):\n",
    "    inputs = data_map_norm[y]\n",
    "    name=str(1992+y)+\"_mapped\"\n",
    "    # map the data with our trained SOM \"mySom\"\n",
    "    myMap = map2d.Map2d(som=mySom, name=name)\n",
    "    myMap.map_from_data(inputs,overwrite=True)\n",
    "    name_err = name + \"_err\"\n",
    "    # print the mapping error \n",
    "    print_error(myMap.relerr, save=save,path=path_fig,filename=name_err)\n",
    "    if surface:\n",
    "        # If we are using observable variable, each element of the vector represents a different type of data\n",
    "        # and should be treated separately\n",
    "        for i in range(nb_data_types_used):\n",
    "            zmin=int(data_min[i])  \n",
    "            zmax=int(data_max[i])+1\n",
    "            \n",
    "            # print SOM output\n",
    "            legend=data_types[data_types_used[i]] + \"-\"+str(1992+y)+\"_mapped\"\n",
    "            name_i = name+\"_\"+data_types[data_types_used[i]]\n",
    "            print_data(myMap.values[:,i],legend=legend,\n",
    "                       save=save, path=path_fig, filename=name_i ,zmin=zmin,zmax=zmax)\n",
    "            \n",
    "            # print the corresponding true values (SOM inputs)\n",
    "            legend=data_types[data_types_used[i]] + \"-\"+str(1992+y)+\"_true_values\"\n",
    "            print_data(destandardize(data_map_norm[y][:,i],data_mean=data_mean[i], data_stdev=data_stdev[i]),\n",
    "                       legend=legend, zmin=zmin,zmax=zmax)\n",
    "    else:\n",
    "        zmin=int(data_min[0])\n",
    "        zmax=int(data_max[0])+1\n",
    "        \n",
    "        # print SOM output\n",
    "        legend=\"temp-\"+str(1992+y)+\"_mapped\"   \n",
    "        print_data(myMap.values, depth, legend=legend,\n",
    "                   save=save,path=path_fig, filename=name,zmin=zmin,zmax=zmax)\n",
    "        \n",
    "        # print the corresponding true values (SOM inputs)\n",
    "        legend=\"temp-\"+str(1992+y)+\"_true_values\"\n",
    "        print_data(destandardize(data_map_norm[y],data_mean=data_mean, data_stdev=data_stdev),\n",
    "                   depth,legend=legend,zmin=zmin,zmax=zmax)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mySom.clear_map_info()\n",
    "\n",
    "inputs = np.concatenate(data_map_norm[:nb_years_train])\n",
    "name=\"1992-\"+str(1992+nb_years_train-1)+\"_mapped\"\n",
    "map_tot = map2d.Map2d(som=mySom,name=name)\n",
    "map_tot.map_from_data(inputs)\n",
    "\n",
    "count = np.count_nonzero(map_tot.som.occ_bmu_map)\n",
    "print(\"Number of classes that were NOT used to map the whole training dataset: \", n*m-count)\n",
    "res = mySom.print_heatmap(data=map_tot.som.occ_bmu_map)\n",
    "\n",
    "# Heatmap of the transition matrix\n",
    "fig= plt.figure(figsize=(15,15))\n",
    "sns.heatmap(mySom.transition, annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are satisfied with the results you can save your Som and Map2d objects! :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=False\n",
    "# Save the SOM with map_info generated only with the training dataset\n",
    "som.save(mySom, path=path_som)\n",
    "\n",
    "save=False\n",
    "# Save a Map2d object for each year of the entire dataset\n",
    "for y in range(nb_years):\n",
    "    name=str(1992+y)+\"_mapped\"\n",
    "    inputs = data_map_norm[y]\n",
    "    newMap = map2d.Map2d(som=som.copy(mySom),name=name)\n",
    "    newMap.map_from_data(inputs)\n",
    "    if save:\n",
    "        map2d.save(newMap, path=path_map)\n",
    "\n",
    "save = False\n",
    "# Save the Map2d corresponding to the entire training dataset  \n",
    "inputs = np.concatenate(data_map_norm[:nb_years_train])\n",
    "name=\"1992-\"+str(1992+nb_years_train-1)+\"_mapped\"\n",
    "map_tot = map2d.Map2d(som=som.copy(mySom),name=name)\n",
    "map_tot.map_from_data(inputs)\n",
    "if save:\n",
    "    map2d.save(map_tot, path=path_map)\n",
    "    \n",
    "save=False\n",
    "# Save the Map2d corresponding to the entire testing dataset  \n",
    "inputs = np.concatenate(data_map_norm[nb_years_train:])\n",
    "name=str(1992+nb_years_train)+\"-2007_mapped\"\n",
    "map_tot = map2d.Map2d(som=som.copy(mySom),name=name)\n",
    "map_tot.map_from_data(inputs)\n",
    "if save:\n",
    "    map2d.save(map_tot, path=path_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
