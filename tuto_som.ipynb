{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import seaborn as sns\n",
    "from submapp import *\n",
    "from tools.tools_som import *\n",
    "from submapp.tools.data_processing import *\n",
    "from submapp.tools.evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.1 Extract data from files\n",
    "\n",
    "There is nothing to change in this cell, just run the cell to extract data from the netcdf4 file. \n",
    "If this does not work, make sure the file ``GotmFabmErsem-BATS.nc`` is in the right folder. (It should be in the same folder as this notebook, ``submapp`` folder and ``tools`` folder )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.1 ##################################################\n",
    "# ---- Extracting data\n",
    "([temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth]) = data_extraction()\n",
    "gotm_data = [temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth]\n",
    "data_types = [\"temp\", \"sst\", \"dswr\", \"airt\", \"ws10\", \"depth\"]\n",
    "\n",
    "for i in range(len(gotm_data)-1):\n",
    "    for y in range(len(gotm_data[i])):\n",
    "        gotm_data[i][y] = mean_steps(gotm_data[i][y],10)\n",
    "[temp_y, sst_y, dswr_y, airt_y, ws10_y ,depth] = gotm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.2 Select data types\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Select the data types you want to work with by initializing the list ``data_types_used`` with some of the following elements \n",
    "     - 0: Temperature (temp)\n",
    "     - 1: Sea surface temperature (sst)\n",
    "     - 2: Incoming Short Wave Radiation (dswr)\n",
    "     - 3: Air Temperature (airt)\n",
    "     - 4: Wind Speed (ws10)\n",
    "     - (-1): Depth (depth)\n",
    "     \n",
    "``Temperature (0)`` is related to the vertical profiles of temperature, in our case they are the \"*hidden*\" values.\n",
    "\n",
    "`` Depth (-1)`` is a vector of levels of depth corresponding to the vertical profiles\n",
    "\n",
    "``Sea surface temperature (1), Incoming Short Wave Radiation (2), Air Temperature (3)`` and ``Wind Speed (4)`` are surface information types. in our case they are the \"*observable*\" values from which we want to infer *hidden* values\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**NOTE**: </span>\n",
    "\n",
    "- If you want to work with *hidden* variables then you should only select ``temperature`` then ``data_types_used = [0]``. \n",
    "- If you want to work with *observale* variables you are not forced to choose all the observable variables! Actually you should NOT use them all as some of them are highly correlated! In our case we will only use ``sst`` and ``dswr`` but feel free to play with other variables as well! :) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.2 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# --- Select your data by initializing data_types_used\n",
    "# - 0: Temperature (temp)\n",
    "# - 1: Sea surface temperature (sst)\n",
    "# - 2: Incoming Short Wave Radiation (dswr)\n",
    "# - 3: Air Temperature (airt)\n",
    "# - 4: Wind Speed (ws10)\n",
    "# - (-1): Depth (depth)\n",
    "data_types_used = [1,2]  # suggested observable variables \n",
    "data_types_used = [0]  # hidden variables\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.3 Split dataset\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Select the number of years in the training dataset (between ``0`` and ``16`` in our case). We suggest you choose ``12``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.3 ##################################################\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select the number of years in the training dataset (SUGGESTED 12)\n",
    "nb_years_train = 12\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# Quick summary -- nothing to do here\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "if 0 in data_types_used:\n",
    "    surface=False\n",
    "    data_y = [gotm_data[0]]\n",
    "    nb_data_types_used = 1\n",
    "    p = depth.size\n",
    "else:\n",
    "    surface=True\n",
    "    nb_data_types_used = len(data_types_used)\n",
    "    data_y = [gotm_data[i] for i in data_types_used]\n",
    "    p = nb_data_types_used\n",
    "    \n",
    "\n",
    "print(\"DATA TYPE USED : \", [data_types[i] for i in data_types_used])\n",
    "print(\"ARE DATA OBSERVABLE: \", surface)\n",
    "\n",
    "nb_years = len(data_y[0])\n",
    "print(\"SIZE OF INPUT VECTORS :\", p)\n",
    "print(\"NUMBER OF YEARS USED FOR TRAINING: \", nb_years_train)\n",
    "print(\"NUMBER OF YEARS USED FOR TESTING: \", nb_years-nb_years_train)\n",
    "print(\"TOTAL NUMBER OF YEARS: \", nb_years)\n",
    "\n",
    "T_y=[]\n",
    "for y in range(nb_years):\n",
    "    T = len(data_y[0][y])\n",
    "    T_y = np.concatenate([T_y, [T]])\n",
    "    print(\"YEAR \", str(1992+y), \" LENGTH : \", T )\n",
    "T_y = T_y.astype(int)\n",
    "print(\"LENGTH OF THE TRAINING TIME SERIES: \", sum(T_y[:nb_years_train]))\n",
    "print(\"LENGTH OF THE TESTING TIME SERIES: \", sum(T_y[nb_years_train:]))\n",
    "print(\"TOTAL LENGTH OF THE TIME SERIES : \", sum(T_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.3.bis [OPT] Emulate missing values\n",
    "\n",
    "If you want to emulate missing values in your datasets you can specify a probability lower than 1 of getting an element for all instant of the time series.\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Specify ``prob``: define the probability ``p`` ( with ``0 <= p <= 1``) of getting a value for each instant ``t``\n",
    "\n",
    "- Specify if this probability should be applied for the training phase set and/or the mapping phase:\n",
    "    - ``complete_data_train = True`` if you want to use the complete dataset for the training phase\n",
    "    - ``complete_data_map = False`` if you want to use the sparse dataset for the mapping phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.3.bis ###############################################\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select the probability of getting data at each instant t\n",
    "prob = 1.\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: \n",
    "# ---- Select if the complete dataset should be used for training/mapping\n",
    "# ---- Or only the sparse dataset\n",
    "complete_data_train = True\n",
    "complete_data_map = False\n",
    "# ----------------------------\n",
    "\n",
    "# --- Remove data randomly\n",
    "complete_data = data_y\n",
    "\n",
    "sparse_data = [[]]*nb_data_types_used\n",
    "for dtype in range(nb_data_types_used):\n",
    "    sparse_data[dtype] = [remove_random_data(data, prob=prob) for data in data_y[dtype]]\n",
    "\n",
    "if complete_data_train:\n",
    "    data_train = complete_data\n",
    "else:\n",
    "    data_train = sparse_data\n",
    "if complete_data_map:\n",
    "    data_map = complete_data\n",
    "else:\n",
    "    data_map = sparse_data\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.4 Standardization\n",
    "\n",
    "There is nothing to do here, run the cell to standardize the data and have a quick summary about the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 0.4 ###############################################\n",
    "\n",
    "# --- Standardization: step 1: find standardization coeff (on the training data set only!)\n",
    "data_train_tot = [np.concatenate(data[:nb_years_train]) for data in data_train]\n",
    "data_mean = np.zeros(nb_data_types_used)\n",
    "data_stdev = np.zeros(nb_data_types_used)\n",
    "data_max = np.zeros(nb_data_types_used)\n",
    "data_min = np.zeros(nb_data_types_used)\n",
    "for i in range(nb_data_types_used):\n",
    "    data_max[i] = np.max(data_train_tot[i])\n",
    "    data_min[i] = np.min(data_train_tot[i])\n",
    "    (_ ,data_mean[i], data_stdev[i]) = standardize(data_train_tot[i])\n",
    "\n",
    "# --- standardization: step 2: standardize\n",
    "data_train_norm = [np.empty([]) for y in range(nb_years)]\n",
    "data_map_norm = [np.empty([]) for y in range(nb_years)]\n",
    "for y in range(nb_years):\n",
    "    if len(data_y[0][0].shape) == 1:\n",
    "        p_tmp=1\n",
    "    else:\n",
    "        (_,p_tmp) = data_y[0][0].shape\n",
    "    data_train_norm[y] = np.concatenate([\n",
    "        np.reshape(standardize(data_train[i][y], \n",
    "                  data_mean[i], \n",
    "                  data_stdev[i]),(T_y[y],p_tmp) ) for i in range(nb_data_types_used)], axis=1)\n",
    "    data_map_norm[y] = np.concatenate([\n",
    "        np.reshape(standardize(data_map[i][y], \n",
    "                  data_mean[i], \n",
    "                  data_stdev[i]),(T_y[y],p_tmp) ) for i in range(nb_data_types_used)], axis=1)\n",
    "    \n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "for i in range(nb_data_types_used):\n",
    "    print(\"DATA TYPE: \", \n",
    "          data_types[data_types_used[i]],\n",
    "          \" MEAN: \", round(data_mean[i],2),\n",
    "          \" STDEV: \", round(data_stdev[i],2),\n",
    "          \" MIN: \", round(data_min[i],2),\n",
    "          \" MAX: \", round(data_max[i],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: SOM configuration and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Choose the shape of the SOM\n",
    "\n",
    "__Shape (n, m)__\n",
    "\n",
    "By *shape of the SOM* we mean here its number of row ``n`` and its number of column ``m``.\n",
    "\n",
    "It is one of the most important parameters and we highly recommend you to test a lot of different shapes. To help you decide on the ratio ``m/n`` you can use the function ``pca_features`` whose first output is the ratio between the 2 first principal components. This ratio can be seen as a suggestion and the second output (the cumulative percentage of explained variance of the first 2 principal components) can be seen as an indicator of relevance of this suggestion. Once again, all this is only about \"*suggestion*\", \"*indication*\" and there is no garantee that this will give the best shape nor even a good one. Once again we highly recommend you to try different shapes!\n",
    "\n",
    "__Total number of classes n*m__\n",
    "\n",
    "The total number of classes ``nb_class=n*m`` matters as well. If this number is too low the SOM will poorly represent the diversity of the data. If this number is too high the SOM may overfit the training data and significantly loose accuracy when it comes to the testing dataset. This is a very common problem in machine learning \n",
    "and there are tons of articles dealing with this issue. If you want to find out more about overfitting, how to detect it and prevent it check out [this article](https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42) by Julien Despois from which the 2 following figures are:\n",
    "<figure>\n",
    "<img src=\"img/overfitting01.png\" alt=\"Recognize underfitting and overfitting\" width=\"600\" />\n",
    "<figcaption> <center> Fig.1 - Recognize underfitting and overfitting </center> </figcaption>\n",
    "<img src=\"img/overfitting02.png\" alt=\"Overfitting and model complexity\" width=\"500\"/>\n",
    "<figcaption> <center> Fig.2 - Overfitting and model complexity </center> </figcaption>\n",
    "</figure>\n",
    "\n",
    "However, in our context, both SOMs we train are to be combined with a HMM. In this situation, the bottom line is the accuracy of the prediction of hidden variables given observable variables and not the accuracy of the SOMs themselves. Another problem here is that the more classes there are, the more parameters there are in the HMM to be estimated. Therefore the more classes there are the more difficult it is to estimate the HMM model, particularly the transition matrix ``Tr`` Indeed, we do not have enough data to estimate properly the probabilities defining its elements. Thus even though both SOMs are not overfitting the training dataset we may have to train new SOMs with fewer classes according to the output of the HMM. \n",
    "\n",
    "The accuracy of the hidden SOM is important though since it defines the best accuracy the HMM can reach. Indeed, in the ideal case your HMM estimates the best classes possible therefore the only error made is the difference between the *true values* and the referent vector representing these *true values* \n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Specify ``n`` the number of rows and ``m`` the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.1 ###############################################\n",
    "\n",
    "#  Apply PCA on the training dataset. \n",
    "# - r: ratio between the first 2 principal components. \n",
    "# - c_v: cumulative variance of the first 2 principal components. \n",
    "r, c_v = pca_features(remove_nan(np.concatenate(data_train_norm[:nb_years_train])))\n",
    "\n",
    "# ----------------------------\n",
    "# TODO:\n",
    "# ---- Select the number of class and the shape of the SOM\n",
    "# For instance (n,m) = (5,80) for hidden variables and (n,m) = (5,15) for observable variables\n",
    "n = 5\n",
    "m = 80\n",
    "# ----------------------------\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"RATIO PC1/PC2: \", round(r,2), \"\\nCUMULATIVE VARIANCE: \", c_v)\n",
    "print(\"SHAPE OF THE SOM :  (\", n, \"x\", m, \")\")\n",
    "print(\"RATIO m/n:  \", round(m/n,2))\n",
    "print(\"TOTAL NUMBER OF CLASSES: \", n*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 SOM initialization\n",
    "\n",
    "###### 1.2.1 SOM name and path\n",
    "\n",
    "If you want to save your ``Som`` object you should customize the name your ``Som`` object and the relative paths at which the objects have to be saved by default to make it easier to load them afterwards. \n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional)</span>\n",
    "\n",
    "- Customize the name of the SOM\n",
    "- Customize the default relative path at which the Som object will be saved\n",
    "- Customize the default relative path at which the figures will be saved\n",
    "- Customize the default relative path at which the Map2d object will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.2 ###############################################\n",
    "\n",
    "############ STEP 1.2.1 ###############################################\n",
    "\n",
    "# Retrieve the name of the data type used to customize the path\n",
    "data_name = \"\"\n",
    "for i in data_types_used:\n",
    "    data_name = data_name + data_types[i] + \"-\" \n",
    "data_name=data_name[:-1]\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: (Optional)\n",
    "# ---- Customize the different default paths\n",
    "path_fig = \"figs/Map/\" + data_name + \"/\" + str(n) + \"-\" + str(m) +\"/\" + str(int(prob*100)) +'/'\n",
    "path_som = \"objects/Som/\" + data_name + \"/\" + str(n) + \"-\" + str(m) +\"/\" + str(int(prob*100)) +'/'\n",
    "path_map = \"objects/Map/\" + data_name + \"/\" + str(n) + \"-\" + str(m) +\"/\" + str(int(prob*100)) +'/'\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: (Optional)\n",
    "# ---- Customize the name of the Som object\n",
    "name=\"1992-\"+str(1992+nb_years_train-1)+\"_trained\"\n",
    "# ----------------------------\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"#################### SUMMARY ###################\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"NAME OF THE SOM: \", name )\n",
    "print(\"FIGS DIRECTORY: \",path_fig)\n",
    "print(\"SOM DIRECTORY: \",path_som)\n",
    "print(\"MAP DIRECTORY: \",path_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.2.2  SOM and weights initialization\n",
    "\n",
    "Once the shape of the SOM is chosen, we can initialize our object ``mySom`` from the class ``Som``.\n",
    "\n",
    "Then weights have to be initialized. The initialization may influence:\n",
    "- The learning speed: \n",
    "  - How many iterations are necessary before getting good results?\n",
    "- The internal organization of the SOM: \n",
    "  - Where are the neurons representing the main features of the data? \n",
    "  - Are they organized in specialized areas? \n",
    "  - Are some areas redundant? \n",
    "  - Are all important features represented by a specialized area?  \n",
    "  - Can we identify clearly the neurons representing the spring, the cool down, etc?\n",
    "- Final performance (after the training)\n",
    "\n",
    "The learning rate and the radius of the neighborhood function can limit this influence. Indeed if both initial learning rate and radius are high, the very first inputs will re-organize the whole SOM almost independently of the initial state of the SOM.  However a good initialization may ensure a stable and coherent internal structure of the SOM, making it a bit less sensitive to the (ir)relevance of the learning rate and radius values chosen. \n",
    "\n",
    "\n",
    "Weights can be initialized in many ways and none of them is better than all others in all cases so once again this is one of the steps in which multiple attempts should be made and analyzed before making a final decision. \n",
    "There are mainly 3 different approaches:\n",
    "- Random initialization: each weight is a sample drawn from a distribution (Gaussian or a uniform for example)\n",
    "- Sample initialization: each weight is a sample drawn from the training dataset\n",
    "- Data analysis based initialization: each weight is spanned by eigenvectors of the input dataset or a referent vector of a K-means method, etc.\n",
    "\n",
    "Here you can use one of these methods\n",
    "- ``random_weights_initialization``: Initializes weights randomly with a gaussian or uniform distribution\n",
    "- ``pca_weights_initialization``: Initializes weights by applying a PCA on athe training dataset\n",
    "- ``sample_weights_initialization``: Initializes weights with a sample of the training data\n",
    "- ``zeros_weights_initialization``: Initializes all weights to zeros vectors\n",
    "\n",
    "Otherwise you can specify your own weights when initializing the Som object\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: </span>\n",
    "\n",
    "- Initialize the weights\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**: (Optional) </span>\n",
    "- Try different initialization and observe the differences by runnning the next cell (step 1.2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.2.2 ###############################################\n",
    "\n",
    "# Initialize the Som object \n",
    "mySom = som.Som(n,m,p,weights=None,name=name,data_mean=data_mean,data_stdev=data_stdev)\n",
    "\n",
    "# ----------------------------\n",
    "# TODO:\n",
    "# ---- Initialize the weights with one of these methods \n",
    "# ---- (or None of them but in that case specify your own weights when initializing the Som object)\n",
    "mySom.random_weights_initialization(distribution=\"gaussian\")\n",
    "mySom.pca_weights_initialization(np.concatenate(data_train_norm[:nb_years_train]))\n",
    "mySom.sample_weights_initialization(np.concatenate(data_train_norm[:nb_years_train]))\n",
    "#mySom.zeros_weights_initialization()\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.2.3 Weights visualization\n",
    "\n",
    "Now that our weights are initiliazed we can try to visualize them\n",
    "\n",
    "These plots can help you understand how weights initialization works but also how hard it is to interpret and visualize weights! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.2.3 ###############################################\n",
    "\n",
    "if not surface or p>1:\n",
    "    # If we are using observable variables, each element of the vector represents a different type of data\n",
    "    # and should be treated separately\n",
    "    if surface:\n",
    "        for i in range(nb_data_types_used):\n",
    "            print(\"DATA TYPE : \", data_types[data_types_used[i]])\n",
    "            print(\"VALUE OF EVERY WEIGHT OF THE SOM \")\n",
    "            weights_mean = mySom.weights_features(features=\"mean\",start=i,end=i+1)\n",
    "            mySom.print_heatmap(weights_mean)\n",
    "    # If we are using hidden variables, each element of the vector represents a different level of depth \n",
    "    # of the temperature vertical profile and we can \"average\" them to make the plotting easier to interpret \n",
    "    else:\n",
    "        print(\"TEMPERATURE, AVERAGE VALUE (ALL LEVELS OF DEPTH) OF EVERY WEIGHT OF THE SOM \")\n",
    "        weights_mean = mySom.weights_features(features=\"mean\")\n",
    "        mySom.print_heatmap(weights_mean)\n",
    "        print(\"TEMPERATURE, STANDARD DEVIATION (ALL LEVELS OF DEPTH) OF EVERY WEIGHT OF THE SOM \")\n",
    "        weights_dev = mySom.weights_features(\"std\")\n",
    "        mySom.print_heatmap(weights_dev)\n",
    "        parts = 4  # the whole profiles will be decomposed into 4 smaller profiles \n",
    "        for i in range(parts):\n",
    "            start=i*p//parts\n",
    "            end=(i+1)*p//parts\n",
    "            print(\"TEMPERATURE, AVERAGE VALUE(LEVELS OF DEPTH\", p-start, \"TO\", p-end,\")\" )\n",
    "            weights_mean = mySom.weights_features(features=\"mean\",start=start,end=end)\n",
    "            mySom.print_heatmap(weights_mean)\n",
    "            print(\"TEMPERATURE, STANDARD DEVIATION (LEVELS OF DEPTH\", p-start, \"TO\", p-end,\")\" )\n",
    "            weights_std = mySom.weights_features(features=\"std\",start=start,end=end)\n",
    "            mySom.print_heatmap(weights_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 SOM training\n",
    "\n",
    "Here a quick reminder about the training phase. \n",
    "\n",
    "```\n",
    "1. Find the Best Matching Unit (BMU): class whose referent vector is the closest to the input\n",
    "2. Update every referent vector: \n",
    "    1. compute the distance between the class associated with the referent vector and the BMU (in the map)\n",
    "    2. update the referent vector so that the closer their associated class is to the BMU (in the map) the more their referent vector will be modified towards the input data.\n",
    "```\n",
    "\n",
    "Thus - for instance - for each instant $t$, given the input vector $x_t$ and knowing that the winning neuron is $c$ the weight $w_{i,j}$ is updated as follows:\n",
    "\n",
    "$$w_{i,j} = w_{i,j} + \\alpha \\times h(w_{i,j},c) \\times [w_{i,j}-x_t] $$\n",
    "\n",
    "with:\n",
    "$$ h(w_{i,j},c) = exp(\\frac{dist(w_{i,j},c)}{ \\sigma^{2} } )$$\n",
    "\n",
    "$$ dist(w_{i_1,j_1},w_{i_2,j_2}) = ||i_1-i_2|| + ||j_1-j_2||$$\n",
    "\n",
    "###### 1.3.1 Choose hyperparameters \n",
    "\n",
    "Before training our SOM we will have to choose (a lot of) hyperparameters. \n",
    "Here a quick description of some of them:\n",
    "\n",
    "- **Learning rate** $\\alpha$: ($0<\\alpha<1$)\n",
    "\n",
    "  Determine the *rate* (or *speed*) of learning. \n",
    "\n",
    "  - If $\\alpha$ is too high: the SOM won't be adaptive but will only reproduce its last training inputs. \n",
    "\n",
    "  - If $\\alpha$ is too low: the SOM won't learn fast enough and the weights won't be representative enough of the data at the end of the training. \n",
    "\n",
    "  The learning rate should decrease over the training. In our case $\\alpha$ decreases linearly after each input vector from ``a0`` to ``aT`` after ``T_train`` inputs but it could have been a exponential decay of an inverse decay as well.\n",
    "  \n",
    "\n",
    "- **Radius** $\\sigma$:  ($0<\\sigma$)\n",
    "\n",
    "  Determine the *radius* of the neighborhood function $h$. The neighborhood function determines how *much* neurons should learn around the *BMU* (Best Matching Unit: the neuron that was the closest to the input vector). Thus in other words, $\\sigma$ determines how *many* neurons will learn around the BMU \n",
    "\n",
    "  - If $\\sigma$ is too high, too many neurons will be activated by a given input through the neighborhood function. As a consequence some neurons might get redundant and therefore useless while some features of the input data will be poorly represented by the (fewer) remaining neurons \n",
    "\n",
    "  - If $\\sigma$ is too low, the different areas will specialize too slowly. It could have different unwanted consequences: \n",
    "    - Multiple small areas corresponding to similar data features instead of a bigger one. It is important that one and only one area exists for a given functionality such as representing the spring because the distance covered in the SOM between the BMU at instant ``t`` and ``t-1`` must be short so that we can use neighboring functions in the HMM. Some rare transitions can be long since some springs might look like autumn sometimes but this must remain an exception!\n",
    "    - Undertraining: some neurons are not trained enough because they are always too far from the *BMU* with respect to the radius.\n",
    "\n",
    "  The radius rate should decrease over the training. In our case $\\sigma$ decreases linearly after each input vector from ``s0`` to ``sT`` after ``T_train`` inputs but it could have been a exponential decay of an inverse decay as well.\n",
    "\n",
    "\n",
    "- **Epochs**:\n",
    "\n",
    "  Determine the number times that the entire training dataset will train the SOM. \n",
    "\n",
    "  During the training phase or once the training is over, plots showing the error with respect to the epochs (sometimes called learning curves) can help to detect overfitting or underfitting. However if the learning rate and the radius decrease enough over the training and if the model is not too complex (total number of classes too high) there should not be overtraining due to a number of epochs too high. \n",
    "\n",
    "- **But also...**\n",
    "\n",
    "  There are more hyperparameters such as the __distance function__ $dist(w_{i_1,j_1},w_{i_2,j_2})$ (parameter ``distance_matrix`` while initializing the Som) which determines whether the SOM is a rectangular or an hexagonal map for instance or the __cost function__ that determines the meaning of \"Best Matching Unit\". However, they are beyond the scope of this tutorial and the idea behind the training remains the same. Here we use a rectangular map with the euclidean norm as cost function.\n",
    "\n",
    "###### 1.3.2 Train\n",
    "\n",
    "Once all the hyperparameters are initialized we can start the training. \n",
    "To do so we use the method ``train`` that updates the weights of the SOM according to the formula above. We repeat the operation ``epochs`` times over the entire training dataset.\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**:</span>\n",
    "\n",
    "- Initialize the number of epochs ``epochs``\n",
    "- Initialize the hyperparameters ``(a0,aT,s0,sT)``\n",
    "- Customize how the learning rate and the radius both decrease over the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ STEP 1.3 ###############################################\n",
    "\n",
    "############ STEP 1.3.1 #############################################\n",
    "# ----------------------------\n",
    "# TODO:\n",
    "# ---- Initialize hyperparameters\n",
    "epochs = 10  # number of epochs\n",
    "a0 = 0.9  # initial learning rate\n",
    "aT = 0.5  # final learning rate (after the first epoch)\n",
    "if surface:\n",
    "    s0 = 5.  # initial radius \n",
    "    sT = 1  # final radius\n",
    "else:\n",
    "    s0 = 20.  # initial radius \n",
    "    sT = 1  # final radius    \n",
    "# ----------------------------\n",
    "\n",
    "param = (a0,aT,s0,sT)\n",
    "T_train = len(np.concatenate(data_train_norm[:nb_years_train]))\n",
    "\n",
    "############ STEP 1.3.2 #############################################\n",
    "for e in range(epochs):\n",
    "    print(\"#### Epoch : \", e, \" ####\")\n",
    "    # A random year is selected from the learning dataset in order not to favor one year over another \n",
    "    for y in random.sample(range(nb_years_train), nb_years_train):\n",
    "        # Train the SOM with one more year from the training dataset \n",
    "        inputs = data_train_norm[y]\n",
    "        mySom.train(\n",
    "            data_train=inputs, \n",
    "            param=param, \n",
    "            T_train=T_train\n",
    "        )\n",
    "        # Map the whole testing dataset to evaluate how the SOM has improved since the last training\n",
    "        inputs = np.concatenate(data_map_norm[nb_years_train:])\n",
    "        mySom.map(data_map=inputs)\n",
    "    # ----------------------------\n",
    "    # TODO: \n",
    "    # ---- Hyperparameters can be updated after each epoch for a better control over the training\n",
    "    a0=aT\n",
    "    aT=max(aT/1.2,0.01)\n",
    "    s0=sT\n",
    "    sT=max(sT/1.2,0.5)\n",
    "    # ----------------------------\n",
    "    param = (a0,aT,s0,sT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4 SOM evaluation\n",
    "\n",
    "Model evaluation corresponds to the question \"Are results satisfying?\". This is a very hard question and even more in our case in which we combine 2 machine learning methods. Indeed, the performance is determined by the estimation of hidden variables given observable variables and not by the accuracy of the SOMs themselves.\n",
    "\n",
    "Besides, one of the objectives of combining SOM and HMM is to use neighboring functions that propagate probabilities or penalize long transitions. However using these functions is relevant only if the internal structure of both SOM is consistent and represent well the topology of the data. As a consequence in our situation, making sure that the different areas within the SOM are all relevant and well organized is of paramount importance. Indeed, given a total number of classes, many different sets of parameters will train SOMs with similar accuracies but only few of them will have a consistent enough internal structure for applying neighborhood functions at the HMM stage. \n",
    "\n",
    "First we will try to ask more specific questions:\n",
    "- Is there a problem of overfitting/underfitting?\n",
    "- Is there a problem of overtraining/undertraining?\n",
    "- Are similar weights close to each other - resulting in specialized areas in the map - as they should in a SOM?\n",
    "- Are some neurons redundant?\n",
    "- Are some areas used too often while some are never used for mapping?\n",
    "- Are outputs of the SOM representative of the inputs?\n",
    "- Are some periods of year well represented while some others are poorly represented?\n",
    "- Can the SOM represent well enough extreme inputs (very high/low temperature? High on the surface and low in the deep subsurface? \n",
    "- Are transitions between the BMU at instant ``t`` and ``t-1`` are mostly short/long?\n",
    "- Is the standard deviation of the distance of transiton low/high?\n",
    "\n",
    "Now that the questions are more specific we can try to find some tools that may answer these questions.\n",
    "\n",
    "###### 1.4.1 Learning curves\n",
    "\n",
    "These plots may give an answer to the following questions:\n",
    "\n",
    "- Is there a problem of overfitting/underfitting?\n",
    "- Is there a problem of overtraining/undertraining?\n",
    "\n",
    "We have already mentioned learning curves while defining the hyperparameter ``epochs``. These curves show the error on both training and testing datasets with respect to the number of training iterations. Here are some cases and their possible explanation: \n",
    "\n",
    "- The error starts to increase on the testing dataset while it is still decreasing on the training dataset: it is probably due to a problem of overfitting (model too complex? learning rate too high? training dataset not representative?)\n",
    "\n",
    "- Both errors are still decreasing at the end of the training phase: it probably means that the model is undertrained (learning rate too low? epoch too low? radius too low?)\n",
    "\n",
    "- Both errors are constant before the end of the training: The model has reached its optimum performance given its complexity before the specified number of epoch? The learning rate is so low that the updating phase has almost no impact on the weights?\n",
    "\n",
    "- Training error should be lower than the testing one!\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**TO DO**:</span>\n",
    "- From now on there is nothing to do but interpreting plots and specifying whether objects and figures should be saved or not. But of course feel free to create new critera or plots! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ STEP 1.4.1 #############################################\n",
    "save = True  # Should we save the following figs? \n",
    "filename_train = mySom.name + \"_err_train\"   # Names of the files of the saved figs\n",
    "filename_test = mySom.name + \"_err_test\"\n",
    "\n",
    "print_error(mySom.relerr_train,legend=\"Learning curve: Train dataset\",save=save, path=path_fig, filename=filename_train)\n",
    "print_error(mySom.relerr_test,legend=\"Learning curve: Test dataset\",save=save, path=path_fig, filename=filename_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.4.2 Weights visualization\n",
    "\n",
    "Here we will plot some features of the weights of the SOM and try to answer some of the previous questions:\n",
    "\n",
    "__Question 1:__\n",
    "Are similar weights close to each other - resulting in specialized areas in the map - as they should in a SOM?\n",
    "- If there are some discontinuities from one weight to its neighbors then the internal structure of the SOM is really really bad.\n",
    "    - A problem with the neighborhood function? => this function should return 1. when applied to the BMU itself, decrease with respect to the distance of the BMU and be positive.\n",
    "    - Wrong initialization of ``distance_matrix`` (defines the distance between classes)? => This function should always be positive, symmetric, and decrease with respect to the distance within the SOM\n",
    "    - learning rate extremely low => The training never really updates the weights\n",
    "    - radius extremely low? Even the direct neighbors are not really affected by the update\n",
    "    \n",
    "\n",
    "- 2 areas share the common features (same average and standard deviation) but are located at 2 different positions in the SOM then there are 2 areas that are speciliazed in the same functionality while there sould be only one.\n",
    "    - A radius too low?\n",
    "    - ratio ``m/n`` not consistent with the data?\n",
    "\n",
    "__Questions 2:__\n",
    "Are some neurons redundant? Are some periods of year well represented while some others are poorly represented?\n",
    "- There is a very large area of extremely similar weights (e.g. representing the winter) while the others areas are \"squeezed\" into the SOM then these neurons might be redundant and winter is probably the only well represented period.\n",
    "    - ratio ``m/n`` not consistent with the data?\n",
    "    - training dataset not representative of a given period? (In case of sparse time series, there could be a lot of missing vectors during the summer for instance resulting in a SOM unable to represent the summer) \n",
    "    \n",
    "<span style=\"background-color: ##FFFF00\">**NOTE**: </span>\n",
    "\n",
    "You can compare these figures with the ones obtained after the weights initialization at step 1.2.3 to see the impact of the training on the SOM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############ STEP 1.4.2 #############################################\n",
    "\n",
    "mySom.clear_map_info()\n",
    "\n",
    "if not surface or p>1:\n",
    "    # If we are using observable variables, each element of the vector represents a different type of data\n",
    "    # and should be treated separately\n",
    "    if surface:\n",
    "        for i in range(nb_data_types_used):\n",
    "            print(\"DATA TYPE : \", data_types[data_types_used[i]])\n",
    "            print(\"VALUE OF EVERY WEIGHT OF THE SOM \")\n",
    "            weights_mean = mySom.weights_features(features=\"mean\",start=i,end=i+1)\n",
    "            mySom.print_heatmap(weights_mean)\n",
    "            \n",
    "    # If we are using hidden variables, each element of the vector represents a different level of depth \n",
    "    # of the temperature vertical profile and we can \"average\" them to make the plotting easier to interpret \n",
    "    else:\n",
    "        print(\"TEMPERATURE, AVERAGE VALUE (ALL LEVELS OF DEPTH) OF EVERY WEIGHT OF THE SOM \")\n",
    "        weights_mean = mySom.weights_features(features=\"mean\")\n",
    "        mySom.print_heatmap(weights_mean)\n",
    "        print(\"TEMPERATURE, STANDARD DEVIATION (ALL LEVELS OF DEPTH) OF EVERY WEIGHT OF THE SOM \")\n",
    "        weights_dev = mySom.weights_features(\"std\")\n",
    "        mySom.print_heatmap(weights_dev)\n",
    "        parts = 4  # the whole profiles will be decomposed into 4 smaller profiles \n",
    "        for i in range(parts):\n",
    "            start=i*p//parts\n",
    "            end=(i+1)*p//parts\n",
    "            print(\"TEMPERATURE, AVERAGE VALUE(LEVELS OF DEPTH\", p-start, \"TO\", p-end,\")\" )\n",
    "            weights_mean = mySom.weights_features(features=\"mean\",start=start,end=end)\n",
    "            mySom.print_heatmap(weights_mean)\n",
    "            print(\"TEMPERATURE, STANDARD DEVIATION (LEVELS OF DEPTH\", p-start, \"TO\", p-end,\")\" )\n",
    "            weights_std = mySom.weights_features(features=\"std\",start=start,end=end)\n",
    "            mySom.print_heatmap(weights_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.4.3 SOM outputs\n",
    "\n",
    "Here are the outputs of our SOM for the complete dataset. The following plots should help to answer these questions:\n",
    "\n",
    "__Question 1:__\n",
    "Is there a problem of overfitting/underfitting?\n",
    "\n",
    "- Compare the results obtained with the training dataset (here the years ``1992-1992+nb_years_train-1``) and the testing dataset (the last ``nb_years-nb_years_train years``). If the first one is far better than the second one then there is a problem of overfitting. \n",
    "    - learning rate too high? => If the learning rate is really close to 1. then the weights can result in a sort of sample the training dataset - which works well to map the training dataset but not to map new inputs.\n",
    "    - ``epochs`` too high?\n",
    "    - total number of classes ``n*m`` too high? => If the total number of classes is really high (close to the number of training inputs) then each weight can be the representant of one training input instead of a representant of one feature in the data, being then less adaptative.\n",
    "\n",
    "__Question 2:__\n",
    "Are outputs of the SOM representative of the inputs?\n",
    "\n",
    "Compare the SOM outputs with the \"real\" data. \n",
    "- If the SOM outputs are not representative enough of the data, even on the training dataset\n",
    "  - ``n*m`` too low? => There is not enough classes, only the very main features are represented by the weights missing a lot of important nuances. \n",
    "- If the SOM outputs are not representative enough but only on the testing dataset then there is a problem of overfitting, cf first question.\n",
    "\n",
    "\n",
    "__Question 3:__\n",
    "Are some periods of year well represented while some others are poorly represented?\n",
    "\n",
    "- Compare the SOM outputs with the \"real\" data for each period of the year. If - for instance - the autumn and winter are always well represented while the summer is only represented with the same classes regardless of the year being mapped then:     \n",
    "    - ratio ``m/n`` not consistent with the data? => If the shape is not consistent with the data, it will be difficult for the areas to specialize according to the different main features of the data. \n",
    "    - training dataset not representative? => In case of sparse time series, there could be a lot of missing vectors during the summer for instance resulting in a SOM unable to represent the summer\n",
    "\n",
    "\n",
    "__Question 4:__\n",
    "Can the SOM represent well enough extreme inputs (very high/low temperature? High on the surface and low in the deep subsurface? )\n",
    "\n",
    "- Compare the SOM outputs with the \"real\" data when the \"real\" data is extreme. If the outputs seem to be capped:\n",
    "    - ratio ``m/n`` not consistent with the data? => (cf previous question)\n",
    "    - total number of classes ``n*m`` too low? => (cf question 2)\n",
    "    - training dataset not representative of extreme values? => (cf previous question)\n",
    "    - learning rate too low? The weights will learn slowly and might be able to represent well a vector that is close to the average (climatology) but no weight can represent extreme values because they take longer to learn.\n",
    "    - radius too high? => Each input vector will affect too many classes around the BMU. As a result all the neurons are an average of too many inputs and can not represent extreme inputs anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ STEP 1.4.3 #############################################\n",
    "mySom.clear_map_info()\n",
    "\n",
    "for y in range(0,nb_years):\n",
    "    inputs = data_map_norm[y]\n",
    "    name=str(1992+y)+\"_mapped\"\n",
    "    # map the data with our trained SOM \"mySom\"\n",
    "    myMap = map2d.Map2d(som=mySom, name=name)\n",
    "    myMap.map_from_data(inputs,overwrite=True)\n",
    "    name_err = name + \"_err\"\n",
    "    # print the mapping error \n",
    "    print_error(myMap.relerr, save=save,path=path_fig,filename=name_err)\n",
    "    if surface:\n",
    "        # If we are using observable variable, each element of the vector represents a different type of data\n",
    "        # and should be treated separately\n",
    "        for i in range(nb_data_types_used):\n",
    "            zmin=int(data_min[i])  \n",
    "            zmax=int(data_max[i])+1\n",
    "            \n",
    "            # print SOM output\n",
    "            legend=data_types[data_types_used[i]] + \"-\"+str(1992+y)+\"_mapped\"\n",
    "            name_i = name+\"_\"+data_types[data_types_used[i]]\n",
    "            print_data(myMap.values[:,i],\n",
    "                       legend=legend, save=save, path=path_fig, filename=name_i ,zmin=zmin,zmax=zmax)\n",
    "            \n",
    "            # print the corresponding true values (SOM inputs)\n",
    "            legend=data_types[data_types_used[i]] + \"-\"+str(1992+y)+\"_true_values\"\n",
    "            print_data(myMap.true_values[:,i],legend=legend,zmin=zmin,zmax=zmax)\n",
    "    else:\n",
    "        zmin=int(data_min[0])\n",
    "        zmax=int(data_max[0])+1\n",
    "        \n",
    "        # print SOM output\n",
    "        legend=\"temp-\"+str(1992+y)+\"_mapped\"   \n",
    "        print_data(myMap.values, \n",
    "                   depth, legend=legend, save=save,path=path_fig, filename=name,zmin=zmin,zmax=zmax)\n",
    "        \n",
    "        # print the corresponding true values (SOM inputs)\n",
    "        legend=\"temp-\"+str(1992+y)+\"_true_values\"\n",
    "        print_data(myMap.true_values, depth,legend=legend,zmin=zmin,zmax=zmax)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.4.4 SOM properties: occurence BMU\n",
    "\n",
    "<span style=\"background-color: ##FFFF00\">**NOTE**:</span>\n",
    "The testing dataset is smaller than the training one, so it is normal if there are more classes that are unused for mapping the testing dataset!\n",
    "\n",
    "__Question 1:__\n",
    "Are some areas used too often while some are never used for mapping?\n",
    "\n",
    "- If some areas (and not only some classes here and there) are not used at all for mapping the entire datasets:\n",
    "  - ratio ``m/n`` not consistent with the data? SOM too thin? => If the SOM is thin, the distance between one side to the opposite one can be very long. It can be a good thing if there are a lot of very different and small areas to train. Otherwise it can result in \"holes\" in the SOM\n",
    "  - total number of classes ``n*m`` too high? => Some areas are never trained because there are enough neurons too represent well the training dataset in others areas. \n",
    "  - radius too low? => The neurons in the unused areas have never been the BMU and they are too far away from the BMU with respect to the radius to be trained enough through the neighborhood function\n",
    "- If the BMU are mostly near the border of the SOM:\n",
    "  - ratio ``m/n`` not consistent with the data? SOM too \"square\"? => Areas that are representing very different inputs tend to repel each other. For the same number of classes, the distance between two areas can be longer in a thin rectangle than in a square\n",
    "  - radius too high? => The radius that are in the center of the map are averaged by too many neurons and only those on the borders or corners are specialized and representative because they are less affected by their neighbors\n",
    "  - learning rate too low? => Neurons are not specialized enough at the end of the training if the learning rate has been too low throughout the training\n",
    "  - training dataset non representative? => If this problem happens only with the testing dataset it might be because there are not such extreme inputs in the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############ STEP 1.4.4 #############################################\n",
    "\n",
    "mySom.clear_map_info()\n",
    "\n",
    "print(\" ########################### OCCURENCE BMU ################################ \\n\")\n",
    "\n",
    "print(\" ########################### TESTING DATASET  ################################\")\n",
    "inputs = np.concatenate(data_map_norm[nb_years_train:])\n",
    "name=str(1992+nb_years_train)+\"-2007_mapped\"\n",
    "map_test = map2d.Map2d(som=mySom,name=name)\n",
    "map_test.map_from_data(inputs)\n",
    "\n",
    "print(\"Number of input vectors mapped: \", map_test.nb_inputs_mapped)\n",
    "print(\"Total number of classes:  \", map_test.nb_class)\n",
    "print(\"Number of classes that were NOT used to map the whole testing dataset: \", n*m-map_test.nb_classes_used)\n",
    "print(\"OCCURENCE BMU: \")\n",
    "res = mySom.print_heatmap(data=map_test.som.occ_bmu_map)\n",
    "\n",
    "print(\" ########################### TRAINING DATASET  ################################\")\n",
    "inputs = np.concatenate(data_map_norm[:nb_years_train])\n",
    "name=\"1992-\"+str(1992+nb_years_train-1)+\"_mapped\"\n",
    "map_train = map2d.Map2d(som=mySom,name=name)\n",
    "map_train.map_from_data(inputs)\n",
    "\n",
    "print(\"Number of input vectors mapped: \", map_train.nb_inputs_mapped)\n",
    "print(\"Total number of classes:  \", map_train.nb_class)\n",
    "print(\"Number of classes that were NOT used to map the whole training dataset: \", n*m-map_train.nb_classes_used)\n",
    "print(\"OCCURENCE BMU: \")\n",
    "res = mySom.print_heatmap(data=map_train.som.occ_bmu_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.4.5 SOM properties: transitions\n",
    "\n",
    "__Question:__\n",
    "Are transitions between the BMU at instant ``t`` and ``t-1`` are mostly short/long?\n",
    "\n",
    "__Question:__\n",
    "Is the standard deviation of the distance of transiton low/high?\n",
    "\n",
    "__Question:__\n",
    "Are similar weights close to each other - resulting in specialized areas in the map - as they should in a SOM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############ STEP 1.4.5 #############################################\n",
    "\n",
    "print(\" ###########################  TRANSITIONS ################################ \\n\")\n",
    "\n",
    "print(\" ########################### TESTING DATASET  ################################\")\n",
    "\n",
    "print(\"Mean distance convered by transitions:\", np.mean(map_test.distance_transitions))\n",
    "print(\"Standard deviation of the distance covered by transitions:\", np.std(map_test.distance_transitions))\n",
    "print(\"DISTANCE TRANSITION: \")\n",
    "plt.plot(map_test.distance_transitions)\n",
    "# Transition matrix\n",
    "fig1= plt.figure(figsize=(15,15))\n",
    "sns.heatmap(map_test.transition, annot=False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\" ########################### TRAINING DATASET  ################################\")\n",
    "\n",
    "print(\"Mean distance convered by transitions:\", np.mean(map_train.distance_transitions))\n",
    "print(\"Standard deviation of the distance covered by transitions:\", np.std(map_train.distance_transitions))\n",
    "print(\"DISTANCE TRANSITION: \")\n",
    "fig=plt.figure()\n",
    "plt.plot(map_train.distance_transitions)\n",
    "# Transition matrix\n",
    "fig2= plt.figure(figsize=(15,15))\n",
    "sns.heatmap(map_train.transition, annot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are satisfied with the results you can save your Som and Map2d objects! :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=True\n",
    "# Save the SOM with map_info generated only with the training dataset\n",
    "mySom.clear_map_info()\n",
    "inputs = np.concatenate(data_map_norm[:nb_years_train])\n",
    "mySom.map(inputs)\n",
    "som.save(mySom, path=path_som)\n",
    "\n",
    "save=True\n",
    "# Save a Map2d object for each year of the entire dataset\n",
    "for y in range(nb_years):\n",
    "    name=str(1992+y)+\"_mapped\"\n",
    "    inputs = data_map_norm[y]\n",
    "    newMap = map2d.Map2d(som=mySom,name=name)\n",
    "    newMap.map_from_data(inputs)\n",
    "    if save:\n",
    "        map2d.save(newMap, path=path_map)\n",
    "\n",
    "save = True\n",
    "# Save the Map2d corresponding to the entire training dataset  \n",
    "inputs = np.concatenate(data_map_norm[:nb_years_train])\n",
    "name=\"1992-\"+str(1992+nb_years_train-1)+\"_mapped\"\n",
    "map_tot = map2d.Map2d(som=mySom,name=name)\n",
    "map_tot.map_from_data(inputs)\n",
    "if save:\n",
    "    map2d.save(map_tot, path=path_map)\n",
    "    \n",
    "save=True\n",
    "# Save the Map2d corresponding to the entire testing dataset  \n",
    "inputs = np.concatenate(data_map_norm[nb_years_train:])\n",
    "name=str(1992+nb_years_train)+\"-2007_mapped\"\n",
    "map_tot = map2d.Map2d(som=mySom,name=name)\n",
    "map_tot.map_from_data(inputs)\n",
    "if save:\n",
    "    map2d.save(map_tot, path=path_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
